{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concept categorization (data, how to define on topic models?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Latest Wikipedia corpus\n",
    "* Extracted plain text\n",
    "* Only used first 1000 words per document\n",
    "* 87,380,300 sentences\n",
    "* 1,813,672,600 words, 9,996,018 unique words\n",
    "\n",
    "## topic model training\n",
    "* Using mallet\n",
    "* 256 topics, 400 iterations, 13 hours\n",
    "\n",
    "## word2vec training\n",
    "* skip-gram model in gensim\n",
    "* 3.5 hours\n",
    "* remove words occurring less than 50 times --> 386,046 words unique words (98 % of original corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-06T19:05:21.193614",
     "start_time": "2016-07-06T19:05:19.700344"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import itertools\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import gensim, logging\n",
    "import pandas as pnd\n",
    "from sklearn.cluster import *\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, RandomizedPCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-06T19:05:21.834859",
     "start_time": "2016-07-06T19:05:21.832209"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:05:20.351812",
     "start_time": "2016-07-04T12:05:20.346542"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render p, div.text_cell_render ul, table.dataframe {\n",
    "font-size:1.3em;\n",
    "line-height:1.1em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:05:21.359079",
     "start_time": "2016-07-04T12:05:21.281133"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data in long form\n",
    "\n",
    "df_topics = pnd.read_csv(\"../code/resources/topics.txt\", header=None)\n",
    "df_topics[\"topic\"] =  df_topics.index\n",
    "df_topics[\"topic_name\"] = df_topics[0]\n",
    "\n",
    "df = pnd.melt(df_topics, id_vars=[\"topic\", \"topic_name\"], var_name=\"position\", value_name=\"word\")\n",
    "df = df[[\"word\", \"topic\", \"topic_name\", \"position\"]]\n",
    "df = df.sort_values(by=[\"topic\", \"position\"]).reset_index(drop=True)\n",
    "df[df.topic == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:05:51.954547",
     "start_time": "2016-07-04T12:05:22.325180"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WORD2VEC_VECTOR_FILE = \"/home/knub/Repositories/master-thesis/data/GoogleNews-vectors-negative300.bin\"\n",
    "GLOVE_VECTOR_FILE = \"/home/knub/Repositories/master-thesis/data/glove.6B.50d.txt\"\n",
    "CBOW_VECTOR_FILE = \"/home/knub/Repositories/master-thesis/data/embedding.model.cbow\"\n",
    "SKIP_GRAM_VECTOR_FILE = \"/home/knub/Repositories/master-thesis/data/embedding.model.skip-gram\"\n",
    "\n",
    "vectors_glove = gensim.models.Word2Vec.load_word2vec_format(GLOVE_VECTOR_FILE, binary=False)\n",
    "vectors_skip = gensim.models.Word2Vec.load_word2vec_format(SKIP_GRAM_VECTOR_FILE, binary=True)\n",
    "vectors_cbow = gensim.models.Word2Vec.load_word2vec_format(CBOW_VECTOR_FILE, binary=True)\n",
    "vectors_default = vectors_skip\n",
    "#vectors_word2vec = gensim.models.Word2Vec.load_word2vec_format(WORD2VEC_VECTOR_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:07:34.128105",
     "start_time": "2016-07-04T12:07:34.045634"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_frame_from_word_vectors(df_param, vectors):\n",
    "    df_param = df_param[df_param[\"word\"].apply(lambda word: word in vectors)]    \n",
    "    df_param[\"embeddings\"] = df_param[\"word\"].apply(lambda word: vectors[word])\n",
    "    return df_param\n",
    "\n",
    "df = get_data_frame_from_word_vectors(df.copy(), vectors_default)\n",
    "df[df.topic == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:07:42.571428",
     "start_time": "2016-07-04T12:07:42.536779"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# financial, muslim, teams in sport, atom physics, math\n",
    "nice_topics = [5, 117, 158, 164, 171]\n",
    "nice_topics = [0, 7, 236]\n",
    "\n",
    "df_part = df[df.topic.apply(lambda topic: topic in nice_topics)].copy()\n",
    "# Show topics of interest\n",
    "df_tmp = pnd.DataFrame(df_part.groupby(\"topic\")[\"word\"].apply(lambda l: l.tolist()).tolist())\n",
    "df_tmp.index = nice_topics\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:07:45.300533",
     "start_time": "2016-07-04T12:07:45.294443"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca(embeddings, n=2):\n",
    "    pca = RandomizedPCA(n_components=n)\n",
    "    return pca.fit_transform(embeddings)\n",
    "\n",
    "def tsne(embeddings):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    return tsne.fit_transform(embeddings)\n",
    "\n",
    "def tsne_with_init_pca(embeddings):\n",
    "    tsne = TSNE(n_components=2, init=\"pca\")\n",
    "    return tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic model in word embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:08:19.613559",
     "start_time": "2016-07-04T12:08:19.600407"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_topics_in_embedding_space(reduction_method, df_param):\n",
    "    embeddings = np.array(df_param[\"embeddings\"].tolist())\n",
    "    X = reduction_method(embeddings)\n",
    "    df_tmp = df_param.copy()\n",
    "    df_tmp[\"x\"] = X[:,0]\n",
    "    df_tmp[\"y\"] = X[:,1]\n",
    "    df_tmp = df_tmp[df_tmp.topic.apply(lambda topic: topic in nice_topics)]\n",
    "    colors = {0: \"red\", 7: \"blue\", 236: \"green\", 164: \"yellow\", 171: \"black\"}\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(df_tmp.x, df_tmp.y, c=df_tmp.topic.apply(lambda topic: colors[topic]), s=80)\n",
    "    \n",
    "    ylim = plt.gca().get_ylim()\n",
    "    step = (ylim[1] - ylim[0]) / 100\n",
    "    \n",
    "    for index, row in df_tmp.iterrows():\n",
    "        plt.text(row.x, row.y - step, row.word, horizontalalignment='center', verticalalignment='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:08:20.801831",
     "start_time": "2016-07-04T12:08:20.795415"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_topics_in_embedding_space(pca, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:08:21.433461",
     "start_time": "2016-07-04T12:08:21.189506"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_topics_in_embedding_space(pca, df_part) # third dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T11:45:13.688568",
     "start_time": "2016-07-04T11:45:13.686100"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plot_topics_in_embedding_space(tsne, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE with PCA initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:08:45.295871",
     "start_time": "2016-07-04T12:08:25.401633"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_topics_in_embedding_space(tsne_with_init_pca, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics from the topic model do not seem to be in similar positions in the vector space, in general.\n",
    "\n",
    "* **Specifity**: The more specific a word is, the closer it is to similar words in the word embedding space. See the \"muslim\", \"islam\", \"mohammad\" cluster.\n",
    "* **Ambiguity**: Ambiguous words are a special problem, for example \"current\" is far away from the other physic terms because it has too many meanings. In fact, it is very close to the word \"new\".\n",
    "* **Context-based similarity**: Topic models can assign different similarities between words based on the context. They are good at finding similar words in a context, which might not be immediately obvious. Example: \"distribution\" is not very similar to \"function\", however in the company of \"mean\", \"probability\", \"data\", \"random\" it is. See also \"Exploring the Space of Topic Coherence Measures\" by Röder et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word embedding similarity of topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg. pairwise similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:12:43.596626",
     "start_time": "2016-07-04T12:12:43.586350"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_pairwise_similarity(words, vectors):\n",
    "    word_pairs = itertools.permutations(words, 2)\n",
    "    similarities = [vectors.similarity(word1, word2) for word1, word2 in word_pairs if word1 < word2]\n",
    "    return np.mean(similarities)\n",
    "\n",
    "def average_top_similarity(words, vectors):\n",
    "    word_pairs = itertools.permutations(words, 2)\n",
    "    similarities = [(word1, vectors.similarity(word1, word2)) for word1, word2 in word_pairs]\n",
    "    max_similarities = [max([s for w, s in l]) for _, l in itertools.groupby(similarities, lambda s: s[0])]\n",
    "    return np.mean(max_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:12:43.920545",
     "start_time": "2016-07-04T12:12:43.906426"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_lengths = list(range(2, 11))\n",
    "def calculate_similarities_for_topic(df_topic, sim_function, vectors):\n",
    "    words_in_topic = df_topic[\"word\"].tolist()\n",
    "    \n",
    "    average_similarities = [sim_function(words_in_topic[:topic_length], vectors)\n",
    "                            for topic_length in topic_lengths]\n",
    "    \n",
    "    return pnd.Series(average_similarities)\n",
    "\n",
    "def calculate_similarity_matrix(sim_function, vectors):\n",
    "    def partial_function(df_topic):\n",
    "        return calculate_similarities_for_topic(df_topic, sim_function, vectors)\n",
    "\n",
    "    df_similarities = df.groupby(\"topic\").apply(partial_function)\n",
    "    df_similarities.columns = [\"%s-words\" % i for i in topic_lengths]\n",
    "    return df_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:12:44.877363",
     "start_time": "2016-07-04T12:12:44.299397"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_similarities = calculate_similarity_matrix(average_pairwise_similarity, vectors_default)\n",
    "df_similarities.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:12:47.615873",
     "start_time": "2016-07-04T12:12:47.467509"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = df_similarities.mean().tolist()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(topic_lengths, means, s=80)\n",
    "plt.title(\"Avg. word similarity (cosine similarity in WE space) of topics up to the nth word\")\n",
    "plt.xlim(0, 11)\n",
    "plt.xticks(list(range(1, 12)))\n",
    "#plt.ylim((0, 0.35))\n",
    "plt.xlabel(\"topic length\")\n",
    "plt.ylabel(\"average similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highest-similar topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-06-29T14:44:14.252669",
     "start_time": "2016-06-29T14:44:14.249025"
    },
    "collapsed": true,
    "variables": {
     "vectors_default.similarity(\"buy\", \"purchase\")": "<p><strong>NameError</strong>: name &#39;vectors_default&#39; is not defined</p>\n",
     "vectors_default.similarity(\"king\", \"prince\")": "<p><strong>NameError</strong>: name &#39;vectors_default&#39; is not defined</p>\n",
     "vectors_default.similarity(\"king\", \"queen\")": "<p><strong>NameError</strong>: name &#39;vectors_default&#39; is not defined</p>\n",
     "vectors_default.similarity(\"topic\", \"topics\")": "<p><strong>NameError</strong>: name &#39;vectors_default&#39; is not defined</p>\n"
    }
   },
   "source": [
    "For comparison, here are a few standard similarities:\n",
    "\n",
    "**king-prince**: {{vectors_default.similarity(\"king\", \"prince\")}}\n",
    "**king-queen**: {{vectors_default.similarity(\"king\", \"queen\")}}\n",
    "**topic-topics**: {{vectors_default.similarity(\"topic\", \"topics\")}}\n",
    "**buy-purchase**: {{vectors_default.similarity(\"buy\", \"purchase\")}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:13:04.178316",
     "start_time": "2016-07-04T12:13:04.174613"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_highest_similar_topics(topic_length, nr_topics=3):\n",
    "    column = \"%s-words\" % topic_length\n",
    "    df_top = df_similarities.sort_values(by=column, ascending=False)[:nr_topics]\n",
    "    return df_top.join(df_topics)[[column] + list(range(topic_length))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:13:06.113337",
     "start_time": "2016-07-04T12:13:06.097175"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_highest_similar_topics(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:13:06.709330",
     "start_time": "2016-07-04T12:13:06.691424"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_highest_similar_topics(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:13:07.232158",
     "start_time": "2016-07-04T12:13:07.214273"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_highest_similar_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Findings\n",
    "\n",
    "* In general, similarity is not very high after the first few words, when comparing against usual similarities\n",
    "* Again, topics with highly specific words have highest WE similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Concept categorization in WE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:13:17.867638",
     "start_time": "2016-07-04T12:13:17.839710"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word(word):\n",
    "    try:\n",
    "        return vectors_default[word]\n",
    "    except:\n",
    "        return vectors_default[\"this\"]\n",
    "\n",
    "df_concept = pnd.read_csv(\n",
    "    \"/home/knub/Repositories/master-thesis/data/concept-categorization/battig_concept-categorization.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None)\n",
    "df_concept.columns = [\"word\", \"concept\"]\n",
    "df_concept[\"embeddings\"] = df_concept[\"word\"].apply(lambda word: get_word(word))\n",
    "df_concept.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T12:13:19.458404",
     "start_time": "2016-07-04T12:13:19.454002"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(df_concept[\"embeddings\"].tolist())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T14:28:24.921131",
     "start_time": "2016-07-04T14:28:24.909357"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stats.stackexchange.com/questions/95731/how-to-calculate-purity\n",
    "def single_cluster_purity(df_tmp):\n",
    "    return df_tmp[\"concept\"].value_counts().max()\n",
    "\n",
    "def calculate_purity(df_with_clusters):\n",
    "    purity = float(sum([single_cluster_purity(df_tmp)\n",
    "                        for _, df_tmp\n",
    "                        in df_with_clusters.groupby(\"cluster_id\")])) / len(df_with_clusters)\n",
    "    return purity\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluate_clustering_algorithm(clustering):\n",
    "    # sim or not sim? PCA or not PCA?\n",
    "    X_sim = metrics.pairwise.pairwise_distances(X, metric=\"cosine\")\n",
    "    clusters = clustering.fit_predict(X_sim)\n",
    "    df_concept[\"cluster_id\"] = clusters\n",
    "    return calculate_purity(df_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-07-04T14:28:25.890055",
     "start_time": "2016-07-04T14:28:25.536613"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clustering in [KMeans(n_clusters=10, init=\"k-means++\", n_jobs=1),\n",
    "                   KMeans(n_clusters=10, init=\"random\", n_jobs=1),\n",
    "                   AgglomerativeClustering(n_clusters=10, linkage=\"ward\"),\n",
    "                   AgglomerativeClustering(n_clusters=10, linkage=\"complete\"),\n",
    "                   AgglomerativeClustering(n_clusters=10, linkage=\"average\"),\n",
    "                   DBSCAN(eps=0.5, min_samples=4),\n",
    "                   DBSCAN(eps=0.5, min_samples=5),\n",
    "                   DBSCAN(eps=0.5, min_samples=7),\n",
    "                   DBSCAN(eps=0.3, min_samples=5),\n",
    "                   DBSCAN(eps=0.9, min_samples=5),\n",
    "                   AffinityPropagation(damping=0.5),\n",
    "                   AffinityPropagation(damping=0.6),\n",
    "                   AffinityPropagation(damping=0.7),\n",
    "                   AffinityPropagation(damping=0.8),\n",
    "                   AffinityPropagation(damping=0.9),\n",
    "                   SpectralClustering(n_clusters=10)]:\n",
    "    print clustering.__class__.__name__\n",
    "    print evaluate_clustering_algorithm(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
