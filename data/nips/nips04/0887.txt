Gradient 
Descent: Second-Order Momentum 
and Saturating Error 
Barak Pearlmutter 
Department of Psychology 
P.O. Box 11A Yale Station 
New Haven, CT 06520-7447 
pearlmutter-barak@yale.edu 
Abstract 
Batch gradient descent, Aw(/) -- -rldE/dw(t), converes to a minimum 
of quadratic form with a time constant no better than Amax/Ami n where 
Ami n and Amax are the minimum and maximum eigenvalues of the Hessian 
matrix of E with respect to w. It was recently shown that adding a 
momentum term Aw(t) ---- -rldE/dw(t ) q-cAw(t- 1) improves this to 
¬ X/Amax/Amin, although only in the batch case. Here we show that second- 
order momentum, Aw(/) -- -rldE/dw(t ) q- cAw(t- 1)q- fiAw(t- 2), can 
lower this no further. We then regard gradient descent with momentum 
as a dynamic system and explore a nonquadratic error surface, showing 
that saturation of the error accounts for a variety of effects observed in 
simulations and justifies some popular heuristics. 
1 INTRODUCTION 
Gradient descent is the bread-and-butter optimization technique in neural networks. 
Some people build special purpose hardware to accelerate gradient descent optimiza- 
tion of backpropagation networks. Understanding the dynamics of gradient descent 
on such surfaces is therefore of great practical value. 
Here we briefly review the known results in the convergence of batch gradient de- 
scent; show that second-order momentum does not give any speedup; simulate a 
real network and observe some effect not predicted by theory; and account for these 
effects by analyzing gradient descent with momentum on a saturating error surface. 
887 
888 Pearlmutter 
1.1 SIMPLE GRADIENT DESCENT 
First, let us review the bounds on the convergence rate of simple gradient descent 
without momentum to a minimum of quadratic form [11, 1]. Let w* be the minimum 
of E, the error, H = d2E/dw2(w*), and Ai, vi be the eigenvalues and eigenvectors 
of H. The weight change equation 
dE 
Aw = --r/dw (1) 
(where Af(t) = f(t + 1)- f(t)) is limited by 
0 < r/< 2/Amax 
(2) 
We can substitute r/= 2/Amax into the weight change equation to obtain conver- 
gence that tightly bounds any achievable in practice, getting a time constant of 
convergence of-1/log(1 - 2s) = (2s) - + O(1), or 
E-E* >- exp(-4st) (3) 
where we use s = Amin/Amax for the inverse eigenvalues spread of H and >- is read 
"asymptotically converges to zero more slowly than." 
1.2 FIRST-ORDER MOMENTUM 
Sometimes a momentum term is used, the weight update (1) being modified to 
incorporate a momentum term c < 1 [5, equation 16], 
Aw(t) = -r/d-(t ) + aAw(t - 1). 
(4) 
The Momentum LMS algorithm, MLMS, has been analyzed by Shynk and Roy [6], 
who have shown that the momentum term can not speed convergence in the online, 
or stochastic gradient, case. In the batch case, which we consider here, Tugay and 
Tanik [9] have shown that momentum is stable when 
c< 1 and O<r/<2(c+l)/Amax (5) 
which speeds convergence to 
by 
E - E* >- exp(-(4v/ + O(s)) t) 
2- 4V/s(1- s ) 
(1 - 2s) 2 
- I = I - 4V5 q- O(s), 
(6) 
r]*-- 2(C* q- 1)/Ama x. (7) 
2 SECOND-ORDER MOMENTUM 
The time constant of asymptotic convergence can be changed from O(Amax/Amin) to 
O(V/Amax/Amin) by going from a first-order system, (1), to a second-order system, 
(4). Making a physical analogy, the first-order system corresponds to a circuit with 
Gradient Descent: Second Order Momentum and Saturating Error 889 
--2 
O.S 
Figure 1: Second-order momentum converges if Omax is less than the value plotted 
as "eta," as a function of a and fl. The region of convergence is bounded by four 
smooth surfaces: three planes and one hyperbola. One of the planes is parallel 
to the r/ axis, even though the sampling of the plotting program makes it appear 
slightly sloped. Another is at r/= 0 and thus hidden. The peak is at 4. 
a resistor, and the second-order system adds a capacitor to make an RC oscillator. 
One might ask whether further gains can be had by going to a third-order system, 
dE 
Aw(t) = -r/ww + aAw(t - 1) + fiAw(t - 2). (8) 
For convergence, all the eigenvalues of the matrix 
Mi = 0 0 1 
-/3 -c + /3 1- rlAi + c 
in ((t- l) (t) (t + 1))'  ((t- ) (t - l) (t))' must have absolute 
value less than or equal to 1, which occurs precisely when 
0 < r I <_ 4(/3+1)/Ai 
,Ai/2- (1- fi) _ o < firlAi/2 + (1- fi). 
For fi _< 0 this is most restrictive for Amax, but for fi > 0 /min also comes into play. 
Taking the limit as Amin - 0, this gives convergence conditions for gradient descent 
with second-order momentum of 
when a _< 3/3+ 1 ß 
0< 
when c >_ 3/3 + 1 ß 
0< 
c 51-/3 
2 
< --(1 + c -/3) 
r/ - Amx 
(9) 
/3+1 ( +/3- 1) 
890 Pearlmutter 
a region shown in figure 1. 
Fastest convergence for Ami n within this region lies along the ridge a = 3fi + 1, 
r/= 2(1 + a - fi)/Amax. Unfortunately, although convergence is slightly faster than 
with first-order momentum, the relative advantage tends to zero as s -- 0, giving 
negligible speedup when Amax >> Amin. For small s, the optimal settings of the 
parameters are 
= 1 - 9vq+ o(s) 
/7., __ 3 
- o(s) 
r/** = 4(1-v/j)+O(s) 
(10) 
where a* is as in (7). 
3 SIMULATIONS 
We constructed a standard three layer backpropagation network with 10 input units, 
3 sigmoidal hidden units, and 10 sigmoidal output units. 15 associations between 
random 10 bit binary input and output vectors were constructed, and the weights 
were initialized to uniformly chosen random values between -1 and +1. Training 
was performed with a square error measure, batch weight updates, targets of 0 and 
1, and a weight decay coefficient of 0.01. 
To get past the initial transients, the network was run at r) - 0.45, c - 0 for 
150 epochs, and at r/: 0.3, c = 0.9 for another 200 epochs. The weights were then 
saved, and the network run for 200 epochs for r/ranging from 0 to 0.5 and c ranging 
from 0 to I from that starting point. 
Figure 3 shows that the region of convergence has the shape predicted by theory. 
Calculation of the eigenvalues of d2E/dw 2 confirms that the location of the bound- 
ary is correctly predicted. Figure 2 shows that momentum speeded convergence by 
the amount predicted by theory. Figure 3 shows that the parameter setting that 
give the most rapid convergence in practice are the settings predicted by theory. 
However, within the region that does not converge to the minimum, there appear 
to be two regimes: one that is characterized by apparently chaotic fluctuations 
of the error, and one which slopes up gradually from the global minimum. Since 
this phenomenon is so atypical of a quadratic minimum in a linear system, which 
either converges or diverges, and this phenomenon seems important in practice, we 
decided to investigate a simple system to see if this behavior could be replicated 
and understood, which is the subject of the next section. 
4 GRADIENT DESCENT WITH SATURATING ERROR 
The analysis of the sections above may be objected to on the grounds that it assumes 
the minimum to have quadratic form and then performs an analysis in the neigh- 
borhood of that minimum, which is equivalent to analyzing a linear unit. Surely 
our nonlinear backpropagation networks are richer than that. 
Gradient Descent: Second Order Momentum and Saturating Error 891 
0.69,5 
0.685 
0.680 
,=.9.25.,7=.o.2 ,rs. <=70 ?=.o.17 
3,50 400 ,*.50 500 550 
epoch 
Figure 2: Error plotted as a function of time for two settings of the learning param- 
eters, both determined empirically: the one that minimized the error the most, and 
the one with  = 0 that minimized the error the most. There exists a less aggressive 
setting of the parameters that converges nearly as fast as the quickly converging 
curve but does not oscillate. 
A clue that this might be the case was shown in figure 3. The region where the 
system converges to the minimum is of the expected shape, but rather than simply 
diverging outside of this region, as would a linear system, more complex phenomena 
are observed, in particular a sloping region. 
Acting on the hypothesis that this region is caused by Amax being maximal at 
the minimum, and gradually decreasing away from it (it must decrease to zero in 
the limit, since the hidden units saturate and the squared error is thus bounded) 
we decided to perform a dynamic systems analysis of the convergence of gradient 
descent on a one dimensional nonquadratic error surface. We chose 
1 
E=I (11) 
l+w 2 
which is shown in figure 4, as this results in a bounded E. 
Letting 
f(w) = w- rlE'(w ) = w(1 - 2r/+ 2w 2 + w 4) (12) 
(1 + w2) = 
be our transfer function, a local analysis at the minimum gives Ama x -' E//(0) ---- 2 
which limits convergence to r/ < 1. Since the gradient towards the minimum is 
always less than predicted by a second-order series at the minimum, such r/are in 
fact globally convergent. As r/passes I the fixedpoint bifurcates into the limit cycle 
w - :J:Fv/- - 1, (13) 
which remains stable until r/- 16/9 = 1.77777..., at which point the single sym- 
metric binary limit cycle splits into two asymmetric limit cycles, each still of period 
two. These in turn remain stable until r/- 2.0732261475-, at which point repeated 
period doubling to chaos occurs. This progression is shown in figure 7. 
892 Pearlmutter 
Figure 3: (Left) the error at epoch 550 as a function of the learning regime. Shading 
is based on the height, but most of the vertical scale is devoted to nonconvergent net- 
works in order to show the mysterious nonconvergent sloping region. The minimum, 
corresponding to the most darkly shaded point, is on the plateau of convergence 
at the location predicted by the theory. (Center) the region in which the network 
is convergent, as measured by a strictly monotonically decreasing error. Learning 
parameter settings for which the error was strictly decreasing have a low value while 
those for which it was not have a high one. The lip at r/= 0 has a value of 0, given 
where the error did not change. The rim at a = 1 corresponds to damped oscillation 
caused by r/> 4aA/(1 -a) 2. (Right) contour plot of the convergent plateau shows 
that the regions of equal error have linear boundaries in the nonoscillatory region 
in the center, as predicted by theory. 
As usual in a bifurcation, w rises sharply as r/ passes 1. But recall that figure 3, 
with the smooth sloping region, plotted the error E rather than the weights. The 
analogous graph here is shown in figure 6 where we see the same qualitative feature 
of a smooth gradual rise, which first begins to jitter as the limit cycle becomes 
asymmetric, and then becomes more and more jagged as the period doubles its way 
to chaos. From figure 7 it is clear that for higher r/the peak error of the attractor 
will continue to rise gently until it saturates. 
Next, we add momentum to the system. This simple one dimensional system du- 
plicates the phenomena we found earlier, as can be seen by comparing figure 3 with 
figure 5. We see that momentum delays the bifurcation of the fixed point attractor 
at the minimum by the amount predicted by (5), namely until r/approaches i q- a. 
At this point the fixedpoint bifurcates into a symmetric limit cycle of period 2 at 
// r/ 1, (14) 
w=+ 
a formula of which (13) is a special case. This limit cycle is stable for 
< 15(1 + .), (15) 
but as r/reaches this limit, which happens at the same time that w reaches q-1/x/ 
(the inflection point of E where E = 1/4) the limit cycle becomes unstable. How- 
ever, for c near I the cycle breaks down more quickly in practice, as it becomes 
haloed by more complex attractors which make it progressively less likely that a 
sequence of iterations will actually converge to the limit cycle in question. Both 
boundaries of this strip, r/ = I + c and r/ = -(1 + c0, are visible in figure 5, 
Gradient Descent: Second Order Momentum and Saturating Error 893 
E 
0 
-3 
0 3 
Figure 4: A one dimen- 
sional tulip-shaped non- 
linear error surface E = 
1 - (1 + w2) 
.l 
0. 
When convergent, the fi- 
nal value is shown; oth- 
erwise E after 100 it- 
Figure 5: E after 50 it- 
erations from a starting 
point of 0.05, as a func- 
tion of r/and . 
erations from a starting 
point of w = 1.0. This a 
more detailed graph of a 
slice of figure 5 at c = 0. 
-'5' 
ß ß 5;. 
,:i 
ß :, 
Figure 7: The attractor in w as a function of r/is shown, with the progression from a 
single attractor at the minimum of E to a limit cycle of period two, which bifurcates 
and then doubles to chaos. a = 0 (left) and a = 0.8 (right). For the numerical 
simulations portions of the graphs, iterations 100 through 150 from a starting point 
of w = i or w = 0.05 are shown. 
particularly since in the region between them E obeys 
E=I_/1+ø 
The bifurcation and subsequent transition to chaos with momentum is shown for 
a = 0.8 in figure 7. This a is high enough that the limit cycle fails to be reached 
by the iteration procedure long before it actually becomes unstable. Note that this 
diagram was made with w started near the minimum. If it had been started far 
from it, the system would usually not reach the attractor at w = 0 but instead 
enter a halo attractor. This accounts for the policy of backpropagation experts, 
who gradually raise momentum as the optimization proceeds. 
894 Pearlmutter 
5 CONCLUSIONS 
The convergence bounds derived assume that the learning parameters are set op- 
timafly. Finding these optimal values in practice is beyond the scope of this pa- 
per, but some techniques for achieving nearly optimal learning rates are available 
[4, 10, 8, 7, 3]. Adjusting the momentum feels easier to practitioners than adjust- 
ing the learning rate, as too high a value leads to small oscillations rather than 
divergence, and techniques from control theory can be applied to the problem [2]. 
However, because error surfaces in practice saturate, techniques for adjusting the 
learning parameters automatically as learning proceeds can not be derived under 
the quadratic minimum assumption, but must take into account the bifurcation and 
limit cycle and the sloping region of the error, or they may mistake this regime of 
stable error for convergence, leading to premature termination. 
References 
[8] 
[9] 
[lO] 
[11] 
[1] S. Thomas Alexander. Adaptive Signal Processing. Springer-Verlag, 1986. 
[2] H. S. Dabis and T. J. Molt. Least mean squares as a control system. Interna- 
tional Journal of Control, 54(2):321-335, 1991. 
[3] Yan Fang and Terrence J. Sejnowski. Faster learning for dynamic recurrent 
backpropagation. Neural Computation, 2(3):270-273, 1990. 
[4] Robert A. Jacobs. Increased rates of convergence through learning rate adap- 
tation. Neural Networks, 1(4):295-307, 1988. 
[5] David E. Rumelhart, Geoffrey E. Hinton, and R. J. Williams. Learning internal 
representations by error propagation. In D. E. Rumelhart, J. L. McClelland, 
and the PDP research group., editors, Parallel distributed processing: Explo- 
rations in the microstructure of cognition, Volume 1: Foundations. MIT Press, 
1986. 
[6] J. J. Shynk and S. Roy. The LMS algorithm with momentum updating. In 
Proceedings of the IEEE International Symposium on Circuits and Systems, 
pages 2651-2654, June 6-9 1988. 
[7] F. M. Silva and L. B. Alineida. Acceleration techniques for the backpropaga- 
tion algorithm. In L. B. Almeida and C. J. Wellekens, editors, Proceedings of 
the 1990 EURASIP Workshop on Neural Networks. Springer-Verlag, February 
1990. (Lecture Notes in Computer Science series). 
Tom Tollenaere. SuperSAB: Fast adaptive back propagation with good scaling 
properties. Neural Networks, 3(5):561-573, 1990. 
Mehmet All Tu/gay and Yalqin Tanik. Properties of the momentum LMS algo- 
rithm. Signal Processing, 18(2):117-127, October 1989. 
T. P. Vogl, J. K. Mangis, A. K. Zigler, W. T. Zink, and D. L. Alkon. Acceler- 
ating the convergence of the back-propagation method. Biological Cybernetics, 
59:257-263, September 1988. 
B. Widrow, J. M. McCool, M. G. Larimore, and C. R. Johnson Jr. Stational and 
nonstationary learning characteristics of the LMS adaptive filter. Proceedings 
of the IEEE, 64:1151-1162, 1979. 
