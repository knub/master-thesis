Automatically generated by Mendeley Desktop 1.17
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Aletras2013,
author = {Aletras, Nikolaos and Stevenson, Mark},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Evaluating Topic Coherence Using Distributional Semantics.pdf:pdf},
journal = {Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013)},
pages = {13--22},
title = {{Evaluating Topic Coherence Using Distributional Semantics}},
year = {2013}
}
@article{Hall2009,
abstract = {Abstract More than twelve years have elapsed since the first public release of WEKA . In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread ... $\backslash$n},
author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H},
doi = {10.1145/1656274.1656278},
file = {:home/knub/Repositories/master-thesis/papers/Other/WEKA.pdf:pdf},
isbn = {1931-0145},
issn = {19310145},
journal = {ACM SIGKDD Explorations},
number = {1},
pages = {10--18},
title = {{The WEKA data mining software}},
url = {http://portal.acm.org/citation.cfm?doid=1656274.1656278{\%}5Cnpapers2://publication/doi/10.1145/1656274.1656278},
volume = {Vol. 11},
year = {2009}
}
@article{Sontag2011a,
abstract = {We consider the computational complexity of probabilistic inference in Latent Dirichlet Allocation (LDA). First, we study the problem of finding the maximum a posteriori (MAP) assignment of topics to words, where the document's topic distribution is integrated out. We show that, when the effective number of topics per document is small, exact inference takes polynomial time. In contrast, we show that, when a document has a large number of topics, finding the MAP assignment of topics to words in LDA is NP-hard. Next, we consider the problem of finding the MAP topic distribution for a document, where the topic-word assignments are integrated out. We show that this problem is also NP-hard. Finally, we briefly discuss the problem of sampling from the posterior, showing that this is NP-hard in one restricted setting, but leaving open the general question.},
author = {Sontag, David and Roy, Daniel},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Complexity of Inference in LDA.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in neural information processing systems},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {1008 -- 1016},
title = {{Complexity of Inference in Latent Dirichlet Allocation}},
url = {http://eprints.pascal-network.org/archive/00008447/},
year = {2011}
}
@article{Chang2009,
author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, J. L. and Blei, David M},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Reading tea leaves.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 22},
pages = {288--296},
title = {{Reading Tea Leaves : How Humans Interpret Topic Models}},
year = {2009}
}
@misc{Das2016,
author = {Das, Rajarshi},
publisher = {Personal communication},
title = {{Personal communication}},
year = {2016}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
annote = {THE standard LDA paper},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Latent Dirichlet Allocation - Blei Ng Jordan.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {lda,topic model},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
url = {http://www.cs.princeton.edu/{~}blei/lda-c/{\%}5Cnpapers2://publication/doi/10.1162/jmlr.2003.3.4-5.993{\%}5Cnpapers2://publication/uuid/4001D0D9-4F9C-4D8F-AE49-46ED6A224F4A{\%}5Cnpapers2://publication/uuid/7D10D5DA-B421-4D94-A3ED-028107B7F9B6{\%}5Cnhttp://www.crossref.org/jmlr},
volume = {Vol. 3},
year = {2003}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:home/knub/Repositories/master-thesis/papers/Other/scikit.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {Vol. 12},
year = {2012}
}
@article{Mikolov2013b,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
annote = {Word Embeddings nicely preserve linear relationships.},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
file = {:home/knub/Repositories/master-thesis/papers/NAACL2013Regularities.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT},
number = {June},
pages = {746--751},
pmid = {1938007},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Linguistic+Regularities+in+Continuous+Space+Word+Representations{\#}0{\%}5Cnhttps://www.aclweb.org/anthology/N/N13/N13-1090.pdf},
volume = {Vol. 13},
year = {2013}
}
@article{Rosner2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1403.6397v1},
author = {Rosner, F},
eprint = {arXiv:1403.6397v1},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Evaluating Topic Coherence Measures.pdf:pdf},
journal = {CoRR},
pages = {1--4},
title = {{Evaluating topic coherence measures}},
year = {2013}
}
@article{Roder2015,
author = {R{\"{o}}der, Michael and Both, Andreas and Hinneburg, Alexander},
doi = {10.1145/2684822.2685324},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Exploring the Space of Topic Coherence Measures.pdf:pdf},
isbn = {9781450333177},
journal = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining - WSDM '15},
keywords = {topic coherence,topic evaluation,topic model},
pages = {399--408},
title = {{Exploring the Space of Topic Coherence Measures}},
url = {http://dl.acm.org/citation.cfm?doid=2684822.2685324},
year = {2015}
}
@article{SparckJones1972,
abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently‐occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
author = {{Sparck Jones}, Karen},
doi = {10.1108/eb026526},
file = {:home/knub/Repositories/master-thesis/papers/LDA/A STATISTICAL INTERPRETATION OF TERM SPECIFITY.pdf:pdf},
issn = {0022-0418},
journal = {Journal of Documentation},
number = {1},
pages = {11--21},
title = {{A Statistical Interpretation of Term Specificity and Its Application in Retrieval}},
url = {http://www.emeraldinsight.com/doi/abs/10.1108/eb026526},
volume = {Vol. 28},
year = {1972}
}
@article{Hofmann1999,
abstract = {Probabilistic Laten t Seman tic Indexing is a no el approac v h to automated documen t indexing whic h is based on a sta? tistical laten t class model for factor analysis of coun t data? Fitted from a training corpus of text documen ts b y a gen? eralization of the Expectation Maximization algorithm? the utilized model is able to deal with domain?speci?c synon ym y as w ell as with polysemous w ords? In con trast to standard Laten t Seman tic Indexing ?LSI? b y Singular V alue Decom? position? the probabilistic v arian t has a solid statistical foun? dation and de?nes a proper generativ e data model? Retriev al experimen ts on a n um ber of test collections indicate sub? stan tial performance gains o er direct term matc v hing meth? odsasw ell as o er LSI? In particular? the com v bination of models with di?eren t dimensionalities has pro en to be ad? v v an tageous? ?},
archivePrefix = {arXiv},
arxivId = {2073829},
author = {Hofmann, Thomas},
doi = {10.1021/ac801303x},
eprint = {2073829},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Probabilistic Latent Semantic Indexing.pdf:pdf},
isbn = {1581130961},
issn = {00032700},
journal = {Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval},
pages = {50--57},
pmid = {18989936},
title = {{Probabilistic latent semantic indexing}},
year = {1999}
}
@article{Zou2013,
abstract = {We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.},
author = {Zou, Will Y and Socher, Richard and Cer, Daniel and Manning, Christopher D},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Bilingual Word Embeddings for Phrase-Based Machine Translation.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)},
pages = {1393--1398},
title = {{Bilingual Word Embeddings for Phrase-Based Machine Translation}},
year = {2013}
}
@article{Newman2010,
abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on point- wise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
author = {Newman, David and Lau, Jh and Grieser, Karl and Baldwin, Timothy},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Automatic Evaluation of Topic Coherence.pdf:pdf},
isbn = {1932432655},
journal = {{\ldots} Language Technologies: The {\ldots}},
number = {June},
pages = {100--108},
title = {{Automatic evaluation of topic coherence}},
url = {http://dl.acm.org/citation.cfm?id=1858011},
year = {2010}
}
@misc{Rennie2008,
author = {Rennie, Jason},
title = {{The 20 Newsgroups data set}},
url = {http://qwone.com/{~}jason/20Newsgroups/},
urldate = {2016-11-22},
year = {2008}
}
@misc{DL4J2016,
author = {{Deeplearning4j Development Team}},
publisher = {Apache Software Foundation License 2.0.},
title = {{Deeplearning4j: Open-source distributed deep learning for the JVM}},
url = {http://deeplearning4j.org}
}
@article{Steele2009,
abstract = {Bayesian methods are widely used for selecting the number of components in a mixture models, in part because frequentist methods have difficulty in addressing this problem in general. Here we compare some of the Bayesianly motivated or justifiable methods for choosing the number of components in a one-dimensional Gaussian mixture model: posterior probabilities for a well-known proper prior, BIC, ICL, DIC and AIC. We also introduce a new explicit unit-information prior for mixture models, analogous to the prior to which BIC corresponds in regular statistical models. We base the comparison on a simulation$\backslash$r$\backslash$nstudy, designed to reflect published estimates of mixture model parameters from the scientific literature across a range of disciplines. We found that BIC clearly outperformed the five other methods, with the maximum a posteriori estimate from the established proper prior second.},
author = {Steele, Russell J. and Raftery, Adrian E.},
file = {:home/knub/Repositories/master-thesis/papers/Other/Performance of Bayesian Model Selection Criteria for.pdf:pdf},
pages = {27 p.},
title = {{Performance of Bayesian Model Selection Criteria for Gaussian Mixture Models}},
year = {2009}
}
@article{Pritchard2000,
abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g. , seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home. html.},
author = {Pritchard, Jonathan K. and Stephens, Matthew and Donnelly, Peter},
doi = {10.1111/j.1471-8286.2007.01758.x},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Inference of Population Structure Using Multilocus Genotype Data.pdf:pdf},
isbn = {0016-6731},
issn = {00166731},
journal = {Genetics},
number = {2},
pages = {945--959},
pmid = {10835412},
title = {{Inference of population structure using multilocus genotype data}},
volume = {Vol. 155},
year = {2000}
}
@article{Goth2016,
abstract = {MARCH 2016 | VOL. 59 | NO. 3 | COMMUNICATIONS OF THE ACM 13 news N O NE OF THE featured speak-ers at the inaugural Text By The Bay conference, held in San Francisco in April 2015, drew laughter when describing a neural network question-answering model that could beat hu-man players in a trivia game. While such performance by com-puters is fairly well known to the general public, thanks to IBM's Wat-son cognitive computer, the speaker, natural language processing (NLP) researcher Richard Socher, said, the neural network model he described " was built by one grad student using deep learning " rather than by a large team with the resources of a global corporation behind them. Socher, now CEO of machine learn-ing developer MetaMind, did not in-tend his remarks to be construed as a comparison of Watson to the academic model he and his colleagues built. As an illustration of the new technical and cultural landscape around NLP, how-ever, the laughter Socher's comment drew was an acknowledgment that ba-sic and applied research in language processing is no longer the exclusive province of those with either deep pockets or strictly academic intentions.},
author = {Goth, Gregory},
doi = {10.1145/2874915},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/NLP Is Breaking Out.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {3},
pages = {13--16},
title = {{Deep or shallow, NLP is breaking out}},
url = {http://dl.acm.org/citation.cfm?doid=2897191.2874915},
volume = {Vol. 59},
year = {2016}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
doi = {10.3115/v1/D14-1162},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/glove.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
pages = {1532--1543},
pmid = {1710995},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@article{Krestel2012,
author = {Krestel, Ralf},
file = {:home/knub/Repositories/master-thesis/papers/Use of Language and Topic Models on the Web - Krestel.pdf:pdf},
title = {{On the Use of Language Models and Topic Models in the Web}},
url = {http://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/?{\_}r=0},
year = {2012}
}
@article{Li2016,
abstract = {Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document.},
archivePrefix = {arXiv},
arxivId = {1606.02979},
author = {Li, Shaohua and Chua, Tat-Seng and Zhu, Jun and Miao, Chunyan},
eprint = {1606.02979},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Generative Topic Embedding.pdf:pdf},
title = {{Generative Topic Embedding: a Continuous Representation of Documents (Extended Version with Proofs)}},
url = {http://arxiv.org/abs/1606.02979},
year = {2016}
}
@article{Levy2014,
abstract = {While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary con- texts. In particular, we perform exper- iments with dependency-based contexts, and show that they produce markedly different embeddings. The dependency- based embeddings are less topical and ex- hibit more functional similarity than the original skip-gram embeddings. 1},
author = {Levy, Omer and Goldberg, Yoav},
doi = {10.3115/v1/P14-2050},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Dependency-Based Word Embeddings.pdf:pdf},
isbn = {9781937284732},
journal = {ACL},
pages = {302--308},
pmid = {1627600},
title = {{Dependency-Based Word Embeddings}},
year = {2014}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/A Neural probabilistic language model.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
volume = {Vol. 3},
year = {2003}
}
@misc{ScalaNlp2009,
author = {{David Hall et al.}},
title = {{ScalaNLP}},
url = {http://www.scalanlp.org/},
year = {2009}
}
@inproceedings{Wang2009,
abstract = {This paper presents PLDA, our parallel implementation of Latent Dirichlet Allocation on MPI and MapReduce. PLDA smooths out storage and computation bottlenecks and provides fault recovery for lengthy distributed computations. We show that PLDA can be applied to large, real-world applications and achieves good scalability. We have released MPI-PLDA to open source at http://code.google.com/p/plda under the Apache License.},
author = {Wang, Yi and Bai, Hongjie and Stanton, Matt and Chen, Wen Yen and Chang, Edward Y.},
booktitle = {International Conference on Algorithmic Applications in Management},
doi = {10.1007/978-3-642-02158-9_26},
file = {:home/knub/Repositories/master-thesis/papers/LDA/PLDA$\backslash$: Parallel Latent Dirichlet Allocation.pdf:pdf},
isbn = {3642021573},
issn = {03029743},
pages = {301--314},
publisher = {Springer},
title = {{PLDA: Parallel latent dirichlet allocation for large-scale applications}},
year = {2009}
}
@article{LiFei-Fei2005,
abstract = {We propose a novel approach to learn and recognize nat- ural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We repre- sent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previ- ous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes with- out supervision. We report satisfactory categorization per- formances on a large set of 13 categories of complex scenes.},
author = {{Li Fei-Fei} and {Pietro Perona}},
doi = {10.1109/CVPR.2005.16},
file = {:home/knub/Repositories/master-thesis/papers/LDA/A Bayesian Hierarchical Model for Learning Natural Scene Categories.pdf:pdf},
isbn = {0-7695-2372-2},
issn = {10636919},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
keywords = {ayesian hierarchical model for,learning natural scene categories},
pages = {524--531},
title = {{A Bayesian Hierarchical Model for Learning Natural Scene Categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1467486},
volume = {Vol. 2},
year = {2005}
}
@inproceedings{Rehurek2010,
author = {Rehurek, Radim and Sojka, Petr},
booktitle = {Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks},
pages = {45--50},
publisher = {ELRA},
title = {{Software Framework for Topic Modelling with Large Corpora}},
year = {2010}
}
@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
annote = {Introducing CBOW and Skip-gram models for computing word embeddings.
https://code.google.com/archive/p/word2vec/

* LDA can also provide word vectors by looking at the topics for a single word, however: "distributed representations perform signifcantly better for preserving linear regularities"
* Also, LDA is very expensive on large data sets
* EVALUATION: Using question pairs, e.g. Paris-France, Berlin-???
* DATASET: Microsoft Sentence Completion Challenge
* EVALUATION: idea: catching out of the list words, e.g. Paris, Berlin, Warsaw, London, Broccoli
* DATASET: SemEval-2012 Task 2: Determine similarity of word pairs A-B and C-D, e.g. dog:bark, cat:meow
* USECASES WE: sentiment analysis, paraphrase detection},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Efficient Estimation of Word Representations in Vector Space - Mikolov et al.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@article{Batmanghelich2016,
abstract = {Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.},
archivePrefix = {arXiv},
arxivId = {1604.00126},
author = {Batmanghelich, Kayhan and Saeedi, Ardavan and Narasimhan, Karthik and Gershman, Sam},
eprint = {1604.00126},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Nonparametric Spherical Topic Modeling with Word Embeddings.pdf:pdf},
journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)},
month = {apr},
title = {{Nonparametric Spherical Topic Modeling with Word Embeddings}},
year = {2016}
}
@misc{Gilks1996,
abstract = {convince readers it is simple and has potential},
author = {Gilks, Walter R. and Richardson, Sylvia and Spiegelhalter, David J.},
booktitle = {Markov Chain Monte Carlo in Practice},
doi = {10.1007/978-1-4899-4485-6_1},
file = {:home/knub/Repositories/master-thesis/papers/Other/MCMC1.pdf:pdf},
isbn = {0412055511},
issn = {0849-6757},
pages = {512},
title = {{Introducing Markov Chain Monte Carlo}},
url = {http://link.springer.com/chapter/10.1007/978-1-4899-4485-6{\_}1},
year = {1996}
}
@article{Sra2012,
abstract = {In high-dimensional directional statistics one of the most basic proba- bility distributions is the von Mises-Fisher (vMF) distribution. Maximum likelihood estimation for the vMF distribution turns out to be surprisingly hard because of a dif- ficult transcendental equation that needs to be solved for computing the concentration parameter $\kappa$. This paper is a followup to the recent paper of Tanabe et al. (Comput Stat 22(1):145–157, 2007), who exploited inequalities about Bessel function ratios to obtain an interval in which the parameter estimate for $\kappa$ should lie; their observa- tion lends theoretical validity to the heuristic approximation of Banerjee et al. (JMLR 6:1345–1382, 2005). Tanabe et al. (Comput Stat 22(1):145–157, 2007) also presented a fixed-point algorithm for computing improved approximations for $\kappa$.However, their approximations require (potentially significant) additional computation, and in this short paper we show that given the same amount of computation as their method, one can achieve more accurate approximations using a truncated Newton method. A more interesting contribution of this paper is a simple algorithm for computing Is(x): the modified Bessel function of the first kind. Surprisingly, our na{\"{i}}ve implementation turns out to be several orders ofmagnitude faster for large arguments common to high- dimensional data, than the standard implementations in well-established software such as Mathematica{\textcopyright}, Maple{\textcopyright}, and Gp/Pari.},
author = {Sra, Suvrit},
doi = {10.1007/s00180-011-0232-x},
file = {:home/knub/Repositories/master-thesis/papers/Other/Kappa Estimation.pdf:pdf},
issn = {09434062},
journal = {Computational Statistics},
keywords = {Bessel ratio,Maximum-likelihood,Modified Bessel function,Numerical approximation,von Mises-Fisher distribution},
number = {1},
pages = {177--190},
title = {{A short note on parameter approximation for von Mises-Fisher distributions: And a fast implementation of I s(x)}},
volume = {Vol. 27},
year = {2012}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Deerwester1990,
abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents ("semantic structure") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising.},
archivePrefix = {arXiv},
arxivId = {arXiv:1403.2923v1},
author = {Deerwester, Scott and Dumais, Susan T. and Harshman, Richard},
doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
eprint = {arXiv:1403.2923v1},
file = {:home/knub/Repositories/master-thesis/papers/Indexing by Latent Semantic Analysis.pdf:pdf},
isbn = {9781450300322},
issn = {0002-8231},
journal = {Journal of the American society for information science},
number = {6},
pages = {391--407},
pmid = {470195},
title = {{Indexing by latent semantic analysis}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Indexing+by+Latent+Semantic+Analysis{\#}0},
volume = {Vol. 41},
year = {1990}
}
@article{Moody2016,
abstract = {Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.},
archivePrefix = {arXiv},
arxivId = {1605.02019},
author = {Moody, Christopher E},
eprint = {1605.02019},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec - Moody.pdf:pdf},
journal = {arXiv preprint arXiv:1605.02019},
title = {{Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec}},
url = {http://arxiv.org/abs/1605.02019},
year = {2016}
}
@article{Bentley1975,
author = {Bentley, Jon Louis},
file = {:home/knub/Repositories/master-thesis/papers/Other/k-d trees.pdf:pdf},
journal = {Communications of the ACM 18.9},
title = {{Multidimensional Binary Search Trees Used for Associative Searching}},
year = {1975}
}
@article{Indyk1998,
author = {Indyk, Piotr and Motwani, Rajeev},
file = {:home/knub/Repositories/master-thesis/papers/Other/LSH.pdf:pdf},
journal = {Proceedings of the thirtieth annual ACM symposium on Theory of computing},
pages = {604--613},
title = {{Approximate Nearest Neighbors : Towards Removing the Curse of Dimensionality RAJEEV MoTwd Department of Computer Science Stanford University}},
year = {1998}
}
@article{Levy2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
doi = {10.1186/1472-6947-15-S2-S2},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Improving Distributional Similarity.pdf:pdf},
isbn = {1472-6947 (Electronic)$\backslash$r1472-6947 (Linking)},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
pmid = {26099735},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
volume = {Vol. 3},
year = {2015}
}
@article{Hinton1986,
abstract = {(From the chapter) first section of this chapter stresses some of the virtues of distributed representations / second section considers the efficiency of distributed representations, and shows clearly why distributed representations can be better than local ones for certain classes of problems / final section discusses some difficult issues which are often avoided by advocates of distributed representations, such as the representation of constituent structure and the sequential focusing of processing effort on different aspects of a structure object (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
annote = {Two theories of representation: distributed vs. undistributed},
author = {Hinton, G E and McClelland, J L and Rumelhart, D E},
doi = {10.1146/annurev-psych-120710-100344},
file = {:home/knub/Repositories/master-thesis/papers/hinton86.pdf:pdf},
isbn = {0-262-68053-X},
issn = {1534-7362},
journal = {Parallel Distributed Processing},
pages = {77--109},
pmid = {21943171},
title = {{Distributed representations}},
year = {1986}
}
@article{Baroni2014,
abstract = {Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts.},
author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, German},
doi = {10.3115/v1/P14-1023},
file = {:home/knub/Repositories/master-thesis/papers/Don't count, predict! A systematic comparison of context-counting vs context-predicting semantic vectors.pdf:pdf},
isbn = {9781937284725},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)},
pages = {238--247},
title = {{Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors}},
year = {2014}
}
@misc{Kullback1951,
author = {Kullback, S. and Leibler, R. A.},
file = {:home/knub/Repositories/master-thesis/papers/Other/Kullback{\_}Leibler{\_}1951.pdf:pdf},
publisher = {The annals of mathematical statistics 22.1},
title = {{On Information And Sufficiency}},
year = {1951}
}
@book{Statistics,
author = {Statistics, Directional},
file = {:home/knub/Repositories/master-thesis/papers/Other/mardia{\&}jupp{\_}2000.pdf:pdf},
isbn = {0471953334},
title = {{No Title}}
}
@misc{Saeedi2016,
author = {Saeedi, Ardavan},
title = {{Personal communication}},
year = {2016}
}
@book{Firth1957,
author = {Firth, John Rupert},
publisher = {Oxford University Press},
title = {{Papers in linguistics, 1934-1951}},
year = {1957}
}
@article{Liu1989,
abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence On uniformly convex problems.},
author = {Liu, Dong C and Nocedal, Jorge},
doi = {10.1007/BF01589116},
file = {:home/knub/Repositories/master-thesis/papers/Other/ON THE LIMITED MEMORY BFGS METHOD FOR LARGE.pdf:pdf},
isbn = {0025-5610},
issn = {0025-5610},
journal = {Mathematical Programming},
keywords = {Large scale nonlinear optimization,conjugate gradient method,limited memory methods,partitioned quasi-Newton method},
pages = {503--528},
title = {{on the Limited M E M O R Y Bfgs M E T H O D for Large Scale O P T I M I Z a T I O N}},
volume = {Vol. 45},
year = {1989}
}
@article{Steyvers2010,
abstract = {Research provides increasing evidence that women and men differ in their decisions to trust. However, information systems research does not satisfactorily explain why these gender differences exist. One possible reason is that, surprisingly, theoretical concepts often do not address the most obvious factor that influences human behavior: biology. Given the essential role of biological factors--and specifically those of the brain--in decisions to trust, the biological influences should naturally include those related to gender. To show empirically that online trust is associated with activity changes in certain brain areas, the authors used functional magnetic resonance imaging (fMRI). In a laboratory experiment, they captured the brain activity of 10 female and 10 male participants simultaneous to decisions on trustworthiness of eBay offers. They found that most of the brain areas that encode trustworthiness differ between women and men. Moreover, they found that women activated more brain areas than did men.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Steyvers, Mark and Griffiths, Tom},
doi = {10.1016/s0364-0213(01)00040-4},
eprint = {1111.6189v1},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Probabilistic Topic Models - SteyversGriffiths.pdf:pdf},
isbn = {1532-4435},
issn = {1386-4564},
journal = {MIS Quarterly},
keywords = {0454,0723,0984,1220,1494,1516,1581 7837,1781 4068 7406 7375 825,2022,2100,2340,2382 7401,2400,2420,2500,2586,2593 677,2598 4068 7406 7375 825,2667,2671 7694,2750,3020,3040,3116 2022,3660,3892 3869 6601 8049 1516,4063 1573,4067 4068 7406 7375 825,4300,454112,4550 8550 1070 5831,4605 1583,4901,4952,5250,5497 7375 825,6213 1516,6242 5831,6613 7375 825,708 5760,7300,7419 5760,7906,8249 6298 7375 825,8390,8444 5760,9130,9190,ATT,Accuracy,Adaptable,Adulthood (18 yrs {\&} older),Aftereffect model,Algorithms,Anomaly detection,Applied,Applied sciences,Astroturf political campaigns,Asymmetric and Private Information (D820),Auctions,Auditory Perception,Automated social engineering,BM25,BNC,BUSINESS ethics,Bayes methods,Bayesian network classifier,Bayesian network classifiers,Behavioral Research,Behavioral sciences,Benchmark testing,Biological Sciences,Bose--Einstein statistics,Botnets,Browsers,Business and Economics,COMMUNITY STRUCTURE,COMPLEX NETWORKS,CONDITIONED response,CONNECTIVITY,CONSUMER education,CONSUMER fraud,CONSUMER protection,Celebrities,Chinese microblog,Classification,Classification algorithms,Clustering,Cognitions,Cognitive Processes,Collective mood,Communication,Communication Systems,Communication and the arts,Communities,Community Networks,Computer Softwa,Computer mediation,Computer science,Computer security,Content spam,Contracts and Reputat,Cooperation,Credibility,Customer relations,Data analysis,Data mining,Data sampling,Data stream,Deception,Decision trees,Detection,Distributed detection,Document clustering,Dyads,Dynamic topic analysis,E-business,EBAY,ELECTRONIC commerce,EMV,Electronic commerce,Electronic mail systems,Emotions,Empirical Study,Employee Attitudes,Employee Interaction,Employees,Employers,Environmental Effects,Environmental Stress,Evolution,Experimental/theoretical,Experimental/theoretical treatment,FALSE advertising,Facebook,Fake reviews,Fake user accounts,Familiarity,Feature extraction,Feature vectors,Feature weighting,Feedback,Female,Filtering,First,Fraud,General Psychology,Genetic algorithms,Germany,Graph centrality,Graph theory,Group {\&} Interpersonal Processes,HIERARCHICAL DIRICHLET PROCESSES,HIGH technology,Heuristic,Hierarchical Dirichlet Process,Human,Humans,INFORMATION resources,INFORMATION resources management,INFORMATION technology,INTERNET advertising,INTERNET marketing,INTERNET users,Image,Immunological,Inference,Information Systems,Information and Internet Services,Information retrieval,Internet,Interpersonal Communication,Interpersonal Influences,Interpersonal Relations,Interpersonal communication,J4.8 algorithm,J48,Katz score,Laplace,Law,LinkedIn,Logistics,Lying,MARKETING,METABOLIC NETWORKS,MODEL,MTIO,Machine Learning,Machine learning,Machine learning algorithms,Male,Malicious users,Malware,Management,Management Personnel,Managers,Mass Media,Mass Media Communications,Models,Monte Carlo simulation,More,Motivation,Natural language processing,Negative selection,New employees,Non-malicious users,Non-parametric topic model,Northern America,ORGANIZATION,Occupational psychology,Okapi BM25,Online Social Networks (OSNs),Online privacy,Online social networks,Ontologies,Ontology,Opinion spam,Organizational Behavior,Organizational behavior,PSYCHOLOGY,Phishing websites,Poisson,Prestige,Probabilistic logic,Problem Solving,Professional Personnel,Professional relationships,Promoter,Proximity,Psychological Theory,Psychological aspects,Psychology: Professional {\&} Research,Public relations,Quantitative Study,RESEARCH,Random variables,Reciprocity,Regression analysis,Reputation,Reputation management,Reputations,Research and Development: Ge,Retail and Wholesale Trade,Retailing,Retailing industry,Review spam,Review spammer detection,SCALE-FREE NETWORKS,SERVICE industries -- Marketing,SMALL-WORLD NETWORKS,SNS features,SVM,Sales,Sales {\&} selling,Second,Semantics,Sentiment analysis,Sentiment tracking,Servers,Service level agreements,Sina Weibo,Social Media,Social Perception,Social Perception {\&} Cognition,Social Sciences,Social Sciences: trends,Social Support,Social aspects,Social conditions {\&} trends,Social media,Social network,Social network analysis,Social network security,Social network service,Social network services,Social networking sites,Social networks,Social research,Social sciences,Social trends {\&} culture,Socialbots,Spam,Spam campaign identification,Spam detection,Spam detection techniques,Spam profile identification,Spamdexing,Spamming,Statistical natural language processing,Stochastic gradient descent,Stock market,Studies,Support vector machines,Survey,Sybil account,Sybil accounts detection,TEXT,TSD,Technological Change,Technology,Telecommunication,Telecommunications systems {\&} Internet communicatio,Text mining,Theory,Topic sentiment analysis,Training,Transactional Relationships,Trust,Trust (Social Behavior),Trust Scale,Truth bias,Twitter,Twitter API,Twitter spam,U 2000,U.S.,URL rate,US,USpam,United States,Unsolicited electronic mail,User Centered Design,User Generated Content,User-generated content,Vectors,Virtual worlds,Visual Perception,WEB,WEB sites,Web 2.0,Web spam,Web spamming prevention techniques,Weibo spam detection method,Willingness to pay,YouTube,[[mkup]]Deception[[mkup]],accounting fraud,ad fraud,ad networks,adaptive deception,adaptive learning paths,affect {\&} cognitions in development of interpersona,affective based trust,amazon,annotations,anomaly detection,ant colony optimization,application,application program interfaces,approximate set similarity,article,artificial neural network,auction,automated Turing tests,automated deception detection,bayesian,bayesian parameter estimation,belief networks,belief propagation,binomial law,bipartite cores,blogs,book,bookitem,card not present,classification,classification method,click spam detection,cliques enumeration,cmc,coalition fraud attacks,coevolutionary algorithms,cognitive based trust,communication technology,computational model,computer network security,consistency of visible {\&} audible characteristics,content-based spam detection frameworks,cooperative behavior,credibility,credit card,crowdsourced manipulation,crowdsourcing,crowdsourcing site,data mining,data sampling,dblp,deceptive review,deep belief network,defection,detecting deception,detection,discrepancy-arousal hypothesis,distrust,document length normalization,e-Commerce (L810),e-business,e-commerce Web sites,e-mail,e-markets,eBay Inc,ecology,effective features,electronic markets,eliteness,email,emerging outbreak monitoring,emnlp2007,environmental psychology,evaluation,expertise location,expressive behavio,facebook,fake identities,fake product reviews,fake reviews,fake web review,feature extraction,file-import-08-07-28,financial fraud,framework,fraud,fraud detection,fraud evidence,fraud prevention,fuzzy logic,graph theory,graphical model,heterogenous networks,iSRD,identity,idf,imbalanced data distribution,imbalanced data distributions,information,information retrieval,information systems,interaction rate,internet advertising,invasive software,large margin classifiers,latent dirichlet allocation,lda,learning (artificial intelligence),learning methods,least-squares,logistic regression,long-surviving Twitter spam accounts,machine learning algorithm,machine learning classifiers,malware,markov random fields,media richness,mercer,meta-memory,metadata,microblog,microblogging social network,microblogging spam detection,multi-instance learning,multiple motives of deception as strategic communi,naive Bayes,on-line intrusion analysis,online advertising,online communities,online customer service,online fraud,online learning,online markets,online shopping process,ontologies (artificial intelligence),opinion mining,opinion spam,opinion spam detection,outlier detection,parisian approach,pattern classification,pattern discovery,penalty distance,perceptrons,plagiarism,plda,post,privacy,probabilistic models,product reviews,promoter,pvnets,randomness,real data experiments,regression,regularity,reputation management,reputation mechanisms,reputation systems,resource,response,retail,retail data processing,retreival,review analysis,review helpfulness,review site,review spam,review spammer,review utility,reviewer behavior,reward distance,scalable algorithms,security,self-nonself space pattern construction,self-similarity,semi-supervised learning,sentiment,sentiment analysis,service quality,shilling,signaling,similarity-sensitive sampling,sir,smoothing,social honeypots,social influence,social media,social media spam,social network security,social networking (online),social networking Websites,social networking services,social networks,social spam,social spam discovery,social-networks,social-spam,spam,spam detection,spam message detection,spam review detection,spammer,spammer identification,spamming behavior,spectrum,splog detection,stepwise,succession law,supervised classification methods,support vector machine,support vector machines,supporting services,supporting services functionality,sweethearting,tag,tag similarity,teiresias,temporal dynamics,term frequency normalization,term weighting,text categorization,text classification,topic model,topic modeling,topology,transactive memory,trust,trust management,trust propagation,trusted computing,twitter,unexpected patterns,unsolicited e-mail,user accounts,user centric ontology,user-generated content,video,video promotion,video response,video spam,web 2.0,web forum,web of trust,web security,web spam detection,wide scale proliferation},
number = {3},
pages = {993--1022},
pmid = {21362469},
title = {{Probabilistic Topic Models}},
url = {http://www.sciencedirect.com/science/article/pii/S0140366413001047{\%}5Cnhttp://ceas.cc/2004/167.pdf{\%}5Cnhttp://doi.acm.org/10.1145/1806338.1806450{\%}5Cnhttp://eprints.soton.ac.uk/272254/{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7033160{\%}5Cnhttp://},
volume = {Vol. 3},
year = {2010}
}
@inproceedings{Maas2011,
abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term–document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment in- formation as well as non-sentiment annota- tions. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
author = {Maas, Andrew L and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
doi = {978-1-932432-87-9},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Learning Word Vectors for Sentiment Analysis.pdf:pdf},
isbn = {9781932432879},
pages = {142--150},
title = {{Learning Word Vectors for Sentiment Analysis}},
year = {2011}
}
@article{Das2015,
abstract = {Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at capturing semantic regularities in language. In this paper we replace LDA's param- eterization of "topics" as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space. This encourages the model to group words that are a priori known to be semantically related into topics. To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decompositions of covariance matrices of the posterior predictive distributions. We further derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis–Hastings step. Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS). Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA. Quantitatively, our technique outperforms existing models at dealing with OOV words in held-out documents.},
author = {Das, Rajarshi and Zaheer, Manzil and Dyer, Chris},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Gaussian LDA for Topic Models with Word Embeddings .pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings ACL 2015},
keywords = {LDA,word embeddings},
pages = {795--804},
title = {{Gaussian LDA for Topic Models with Word Embeddings}},
year = {2015}
}
@article{Navigli2015,
author = {Navigli, Roberto},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/A Framework for an Intrinsic Evaluation of Word Vector Representations.pdf:pdf},
title = {{Find the word that does not belong : A Framework for an Intrinsic Evaluation of Word Vector Representations}},
year = {2015}
}
@article{Griffiths2004,
abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. {\&} Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying "hot topics" by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
author = {Griffiths, Thomas L and Steyvers, Mark},
doi = {10.1073/pnas.0307752101},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Finding Scientific Topics - Griffiths Steyers.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Databases,Documentation,Factual,Models,Monte Carlo Method,National Academy of Sciences (U.S.),Probability,Publishing,Science,Science: classification,Statistical,United States},
pages = {5228--35},
pmid = {14872004},
title = {{Finding scientific topics}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14872004},
volume = {Vol. 101},
year = {2004}
}
@misc{McCallum2002,
author = {McCallum, Kachites Andrew},
title = {{MALLET: A Machine Learning for Language Toolkit}},
url = {http://mallet.cs.umass.edu/},
year = {2002}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Distributed Representations of Words and Phrases and their Compositionality - Mikolov et al.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Nips},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Cheng2015,
author = {Cheng, Jianpeng and Wang, Zhongyuan and Wen, Ji-Rong and Yan, Jun and Chen, Zheng},
doi = {10.1145/2806416.2806517},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Contextual Text Understanding in Distributional Semantic.pdf:pdf},
isbn = {9781450337946},
journal = {Proceedings of the 24th ACM International Conference on Information and Knowledge Management},
keywords = {information retrieval,knowledge representation,machine learn-},
pages = {133--142},
title = {{Contextual Text Understanding in Distributional Semantic Space}},
url = {http://dl.acm.org/citation.cfm?doid=2806416.2806517},
year = {2015}
}
@misc{Mikolov2013d,
author = {Mikolov, Tomas},
pages = {https://groups.google.com/forum/{\#}!msg/word2vec--too},
title = {{Tomas Mikolov on preprocessing for word2vec}},
year = {2013}
}
@article{Nguyen2015,
abstract = {Probabilistic topic models are widely used to discover latent topics in document collec- tions, while latent feature vector representa- tions of words have been used to obtain high performance in many NLP tasks. In this pa- per, we extend two different Dirichlet multino- mial topic models by incorporating latent fea- ture vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus. Exper- imental results show that by using informa- tion from the external corpora, our new mod- els produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.},
author = {Nguyen, Dat Quoc and Billingsley, Richard and Du, Lan and Johnson, Mark},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Improving Topic Models with Latent Feature Word Representations.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {299--313},
title = {{Improving topic models with latent feature word representations}},
volume = {Vol. 3},
year = {2015}
}
@article{Nigam2000,
abstract = {This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled docu- ments. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30{\%}.},
author = {Nigam, Kamal and McCallum, Andrew Kachites and Thrun, Sebastian and Mitchell, Tom},
doi = {10.1023/A:1007692713085},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Text Classification from Labeled and Unlabeled.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Bayesian learning,Expectation-Maximization,combining labeled and unlabeled data,integrating supervised and unsupervised learning,text classification},
number = {2-3},
pages = {103--134},
pmid = {15084654},
title = {{Text Classification from Labeled and Unlabeled Documents using EM}},
url = {http://dl.acm.org/citation.cfm?id=347709.347724},
volume = {Vol. 39},
year = {2000}
}
@article{Agirre2009,
abstract = {This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.},
author = {Agirre, Eneko and Alfonseca, Enrique and Hall, Keith and Kravalova, Jana and Pas, Marius and Soroa, Aitor},
doi = {10.3115/1620754.1620758},
file = {:home/knub/Repositories/master-thesis/papers/Other/A study on similarity and relatedness using distributional and WordNet-based approaches.pdf:pdf},
isbn = {978-1-932432-41-1},
issn = {1351-3249},
journal = {Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT 2009)},
pages = {19--27},
title = {{A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches}},
year = {2009}
}
@article{Boyd-graber2014,
abstract = {Topic models are a versatile tool for understanding corpora, but they are not perfect. In this chapter, we describe the problems users often encounter when using topic models for the first time. We begin with the preprocessing choices users must make when creating a corpus for topic modeling for the first time, followed by options users have for running topic models. After a user has a topic model learned from data, we describe how users know whether they have a good topic model or not and give a summary of the common problems users have, and how those problems can be addressed and solved by recent advances in both models and tools.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Boyd-Graber, Jordan and Mimno, David and Newman, David},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/knub/Repositories/master-thesis/papers/LDA/CareAndFeedingOfTopicModels.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Handbook of Mixed Membership Models and Its Applications},
pages = {225 -- 254},
pmid = {25246403},
title = {{Care and Feeding of Topic Models: Problems, Diagnostics, and Improvementes}},
year = {2014}
}
@misc{Six2016,
author = {Six, Joren},
title = {{TarsosLSH}},
url = {https://github.com/JorenSix/TarsosLSH}
}
@article{Liu2015,
abstract = {Most word embedding models typically represent each word using a single vector, which makes these models indiscriminative for ubiquitous homonymy and polysemy. In order to enhance discriminativeness, we employ latent topic models to assign topics for each word in the text corpus, and learn topical word embeddings (TWE) based on both words and their topics. In this way, contextual word embeddings can be flexibly obtained to measure contextual word similarity. We can also build document representations, which are more expressive than some widely-used document models such as latent topic models. In the experiments, we evaluate the TWE models on two tasks, contextual word similarity and text classification. The experimental results show that our models outperform typical word embedding models including the multi-prototype version on contextual word similarity, and also exceed latent topic models and other representative document models on text classification. The source code of this paper can be obtained from https://github.com/ largelymfs/topical{\_}word{\_}embeddings.`},
author = {Liu, Yang and Liu, Zhiyuan and Chua, Tat-Seng and Sun, Maosong},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Topical Word Embeddings.pdf:pdf},
isbn = {9781577357018},
journal = {Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI'15)},
keywords = {NLP and Text Mining Track},
number = {C},
pages = {2418--2424},
title = {{Topical Word Embeddings}},
volume = {Vol. 2},
year = {2015}
}
@book{Bird2009,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bird, Steven and Klein, Ewan and Loper, Edward},
booktitle = {PhD Proposal},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/knub/Repositories/master-thesis/papers/Other/Natural Language Processing with Python (2009).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
publisher = {O'Reilly Media Inc.},
title = {{Natural Language Processing with Python}},
volume = {Vol. 1},
year = {2009}
}
@article{Newman2009,
abstract = {We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straightforward mapping of LDA to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled LDA. The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both LDA and HDP, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.},
author = {Newman, David and Welling, Max},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Distributed Algorithms for Topic Models.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {distributed,hierarchical dirichlet processes,latent dirichlet allocation,parallel computation,topic models},
pages = {1801--1828},
title = {{Distributed Algorithms for Topic Models}},
url = {http://portal.acm.org/citation.cfm?id=1755845},
volume = {Vol. 10},
year = {2009}
}
@article{Schwarz1978,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
file = {:home/knub/Repositories/master-thesis/papers/Other/Estimating the Dimension of a Model.pdf:pdf},
isbn = {0780394224},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
pages = {461--464},
pmid = {2958889},
title = {{Estimating the dimension of a model}},
url = {http://projecteuclid.org/euclid.aos/1176344136},
volume = {Vol. 6},
year = {1978}
}
@article{Levy2014a,
abstract = {Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.'s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.},
author = {Levy, Omer and Goldberg, Yoav},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf:pdf},
isbn = {9781941643020},
journal = {CoNLL},
pages = {171--180},
title = {{Linguistic regularities in sparse and explicit word representations}},
url = {http://anthology.aclweb.org/W/W14/W14-16.pdf{\#}page=181},
year = {2014}
}
@article{Rumelhart1988,
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
file = {:home/knub/Repositories/master-thesis/papers/ADA164453.pdf:pdf},
journal = {Cognitive modeling},
pages = {1},
title = {{Learning representations by back-propagating errors}},
volume = {Vol. 5},
year = {1988}
}
