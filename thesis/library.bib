Automatically generated by Mendeley Desktop 1.17
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Batmanghelich2016,
abstract = {Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.},
archivePrefix = {arXiv},
arxivId = {1604.00126},
author = {Batmanghelich, Kayhan and Saeedi, Ardavan and Narasimhan, Karthik and Gershman, Sam},
eprint = {1604.00126},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Nonparametric Spherical Topic Modeling with Word Embeddings.pdf:pdf},
month = {apr},
title = {{Nonparametric Spherical Topic Modeling with Word Embeddings}},
url = {http://arxiv.org/abs/1604.00126},
year = {2016}
}
@article{Resnik2009,
abstract = {This document is intended for computer scientists who would like to try out a Markov Chain Monte Carlo (MCMC) technique, particularly in order to do inference with Bayesian models on problems related to text processing. We try to keep theory to the absolute minimum needed, and we work through the details much more explicitly than you usually see even in “introductory” explanations. That means we've attempted to be ridiculously explicit in our exposition and notation. After providing the reasons and reasoning behind Gibbs sampling (and at least nodding our heads in the direction of theory), we work through two applications in detail. The first is the derivation of a Gibbs sampler for Naive Bayes models, which illustrates a simple case where the math works out very cleanly and it's possible to “integrate out” the model's continuous parameters to build a more efficient algorithm. The second application derives the Gibbs sampler for a model that is similar to Naive Bayes, but which adds an additional latent variable. Having gone through the two examples, we discuss some practical implementation issues. We conclude with some pointers to literature that we've found to be somewhat more friendly to uninitiated readers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Resnik, P and Resnik, P and Hardisty, E and Hardisty, E},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Gibbs Sampling for the Unitiated - Resnik Hardisty.pdf:pdf},
isbn = {0198553595},
issn = {{\textless}null{\textgreater}},
journal = {Umiacs.Umd.Edu},
keywords = {bayesian inference,gibbs sampling,markov chain monte carlo,na,ıve bayes},
number = {June},
pages = {1--23},
pmid = {25246403},
title = {{Gibbs Sampling for the Uninitiated}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Gibbs+Sampling+for+the+Uninitiated{\#}1},
year = {2009}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Distributed Representations of Words and Phrases and their Compositionality - Mikolov et al.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Nips},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@inproceedings{Huang2008,
author = {Huang, Eric H and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Improving word representations via global context and multiple word prototypes.pdf:pdf},
pages = {873--882},
title = {{Improving Word Representations via Global Context and Multiple Word Prototypes}},
year = {2008}
}
@article{Levy2014,
abstract = {While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary con- texts. In particular, we perform exper- iments with dependency-based contexts, and show that they produce markedly different embeddings. The dependency- based embeddings are less topical and ex- hibit more functional similarity than the original skip-gram embeddings. 1},
author = {Levy, Omer and Goldberg, Yoav},
doi = {10.3115/v1/P14-2050},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Dependency-Based Word Embeddings.pdf:pdf},
isbn = {9781937284732},
journal = {ACL},
pages = {302--308},
pmid = {1627600},
title = {{Dependency-Based Word Embeddings}},
year = {2014}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:home/knub/Repositories/master-thesis/papers/Other/scikit.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Rong,
author = {Rong, Xin},
file = {:home/knub/Repositories/master-thesis/papers/w2vexp.pdf:pdf},
pages = {1--21},
title = {{word2vec Parameter Learning Explained Continuous Bag-of-Word Model}}
}
@article{Steele2009,
abstract = {Bayesian methods are widely used for selecting the number of components in a mixture models, in part because frequentist methods have difficulty in addressing this problem in general. Here we compare some of the Bayesianly motivated or justifiable methods for choosing the number of components in a one-dimensional Gaussian mixture model: posterior probabilities for a well-known proper prior, BIC, ICL, DIC and AIC. We also introduce a new explicit unit-information prior for mixture models, analogous to the prior to which BIC corresponds in regular statistical models. We base the comparison on a simulation$\backslash$r$\backslash$nstudy, designed to reflect published estimates of mixture model parameters from the scientific literature across a range of disciplines. We found that BIC clearly outperformed the five other methods, with the maximum a posteriori estimate from the established proper prior second.},
author = {Steele, Russell J. and Raftery, Adrian E.},
file = {:home/knub/Repositories/master-thesis/papers/Other/Performance of Bayesian Model Selection Criteria for.pdf:pdf},
pages = {27 p.},
title = {{Performance of Bayesian Model Selection Criteria for Gaussian Mixture Models}},
year = {2009}
}
@book{Bird2009,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bird, Steven and Klein, Ewan and Loper, Edward},
booktitle = {PhD Proposal},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/knub/Repositories/master-thesis/papers/Other/Natural Language Processing with Python (2009).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
publisher = {O'Reilly Media Inc.},
title = {{Natural Language Processing with Python}},
volume = {1},
year = {2009}
}
@misc{McCallum2002,
author = {McCallum, Kachites Andrew},
title = {{MALLET: A Machine Learning for Language Toolkit}},
url = {http://mallet.cs.umass.edu/},
year = {2002}
}
@misc{Saeedi2016,
author = {Saeedi, Ardavan},
title = {{Personal communication}},
year = {2016}
}
@article{Finkelstein2002,
abstract = {We describe a new paradigm for performing search in context. In the IntelliZap system we developed, search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (“the context”). The involves context-guided semantic keyword extraction and clustering information retrieval process to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. The results are then semantically reranked, again, using context. It is our belief that letting context guide the search provides a better match to the user's current needs than just relying on the user's fixed personal profile. Our guide search effectively offers even inexperienced users an advanced search tool on the Web},
author = {Finkelstein, Lev and Gabrilovich, Evgeniy and Matias, Yossi and Rivlin, Ehud and Solan, Zach and Wolfman, Gadi and Ruppin, Eytan},
doi = {10.1145/503104.503110},
file = {:home/knub/Repositories/master-thesis/papers/Datasets/WordSim353.pdf:pdf},
isbn = {1581133480},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
keywords = {-based search engines are,a considerable amount of,although such systems seem,as a popular means,conceptual paradigm for performing,deceptively simple,for web-based,in widespread use today,information needs,information retrieval,largely automates the search,new,order to satisfy non-trivial,process,search in context that,skill is required in,this paper presents a},
number = {1},
pages = {116--131},
title = {{Placing search in context: the concept revisited}},
url = {http://www.cs.technion.ac.il/{~}gabr/papers/tois{\_}context.pdf},
volume = {20},
year = {2002}
}
@article{Liu1989,
abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence On uniformly convex problems.},
author = {Liu, Dong C and Nocedal, Jorge},
doi = {10.1007/BF01589116},
file = {:home/knub/Repositories/master-thesis/papers/Other/ON THE LIMITED MEMORY BFGS METHOD FOR LARGE.pdf:pdf},
isbn = {0025-5610},
issn = {0025-5610},
journal = {Mathematical Programming},
keywords = {Large scale nonlinear optimization,conjugate gradient method,limited memory methods,partitioned quasi-Newton method},
pages = {503--528},
title = {{on the Limited M E M O R Y Bfgs M E T H O D for Large Scale O P T I M I Z a T I O N}},
volume = {45},
year = {1989}
}
@article{Indyk1998,
author = {Indyk, Piotr and Motwani, Rajeev},
file = {:home/knub/Repositories/master-thesis/papers/Other/LSH.pdf:pdf},
journal = {Proceedings of the thirtieth annual ACM symposium on Theory of computing},
pages = {604--613},
title = {{Approximate Nearest Neighbors : Towards Removing the Curse of Dimensionality RAJEEV MoTwd Department of Computer Science Stanford University}},
year = {1998}
}
@article{Das2015,
abstract = {Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at capturing semantic regularities in language. In this paper we replace LDA's param- eterization of "topics" as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space. This encourages the model to group words that are a priori known to be semantically related into topics. To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decompositions of covariance matrices of the posterior predictive distributions. We further derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis–Hastings step. Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS). Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA. Quantitatively, our technique outperforms existing models at dealing with OOV words in held-out documents.},
author = {Das, Rajarshi and Zaheer, Manzil and Dyer, Chris},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Gaussian LDA for Topic Models with Word Embeddings .pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings ACL 2015},
keywords = {LDA,word embeddings},
pages = {795--804},
title = {{Gaussian LDA for Topic Models with Word Embeddings}},
year = {2015}
}
@article{Moody2016,
abstract = {Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.},
archivePrefix = {arXiv},
arxivId = {1605.02019},
author = {Moody, Christopher E},
eprint = {1605.02019},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec - Moody.pdf:pdf},
journal = {arXiv preprint arXiv:1605.02019},
title = {{Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec}},
url = {http://arxiv.org/abs/1605.02019},
year = {2016}
}
@article{Bentley1975,
author = {Bentley, Jon Louis},
file = {:home/knub/Repositories/master-thesis/papers/Other/k-d trees.pdf:pdf},
journal = {Communications of the ACM 18.9},
title = {{Multidimensional Binary Search Trees Used for Associative Searching}},
year = {1975}
}
@article{Griffiths2004,
abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. {\&} Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying "hot topics" by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
author = {Griffiths, Thomas L and Steyvers, Mark},
doi = {10.1073/pnas.0307752101},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Finding Scientific Topics - Griffiths Steyers.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Databases, Factual,Documentation,Models, Statistical,Monte Carlo Method,National Academy of Sciences (U.S.),Probability,Publishing,Science,Science: classification,United States},
pages = {5228--35},
pmid = {14872004},
title = {{Finding scientific topics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14872004},
volume = {101 Suppl },
year = {2004}
}
@book{Firth1957,
author = {Firth, John Rupert},
publisher = {Oxford University Press},
title = {{Papers in linguistics, 1934-1951}},
year = {1957}
}
@article{Heinrich2008,
abstract = {Presents parameter estimation methods common with discrete proba- bility distributions, which is of particular interest in text modeling. Starting with maximum likelihood, a posteriori and Bayesian estimation, central concepts like conjugate distributions and Bayesian networks are reviewed. As an application, the model of latent Dirichlet allocation (LDA) is explained in detail with a full derivation of an approximate inference algorithm based on Gibbs sampling, in- cluding a discussion of Dirichlet hyperparameter estimation.},
author = {Heinrich, Gregor},
doi = {10.2514/2.3375},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Parameter Estimation for Text Analysis - Heinrich.pdf:pdf},
issn = {00224650},
journal = {Web: http://www. arbylon. net/publications/text-est. pdf},
pages = {1--31},
title = {{Parameter Estimation for Text Analysis}},
url = {http://www.arbylon.net/publications/text-est2.pdf},
year = {2008}
}
@article{Boyd-graber2014,
abstract = {Topic models are a versatile tool for understanding corpora, but they are not perfect. In this chapter, we describe the problems users often encounter when using topic models for the first time. We begin with the preprocessing choices users must make when creating a corpus for topic modeling for the first time, followed by options users have for running topic models. After a user has a topic model learned from data, we describe how users know whether they have a good topic model or not and give a summary of the common problems users have, and how those problems can be addressed and solved by recent advances in both models and tools.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Boyd-graber, Jordan and Mimno, David and Newman, David},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/knub/Repositories/master-thesis/papers/LDA/CareAndFeedingOfTopicModels.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Handbook of Mixed Membership Models and Its Applications},
pages = {225 -- 254},
pmid = {25246403},
title = {{Care and Feeding of Topic Models: Problems, Diagnostics, and Improvementes}},
year = {2014}
}
@article{Brochu2010,
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, E and Cora, V M and {De Freitas}, N},
eprint = {1012.2599},
file = {:home/knub/Repositories/master-thesis/papers/Other/A Tutorial on Bayesian Optimization of.pdf:pdf},
journal = {Rl},
pages = {1--49},
title = {{A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning}},
year = {2010}
}
@article{Sra2012,
abstract = {In high-dimensional directional statistics one of the most basic proba- bility distributions is the von Mises-Fisher (vMF) distribution. Maximum likelihood estimation for the vMF distribution turns out to be surprisingly hard because of a dif- ficult transcendental equation that needs to be solved for computing the concentration parameter $\kappa$. This paper is a followup to the recent paper of Tanabe et al. (Comput Stat 22(1):145–157, 2007), who exploited inequalities about Bessel function ratios to obtain an interval in which the parameter estimate for $\kappa$ should lie; their observa- tion lends theoretical validity to the heuristic approximation of Banerjee et al. (JMLR 6:1345–1382, 2005). Tanabe et al. (Comput Stat 22(1):145–157, 2007) also presented a fixed-point algorithm for computing improved approximations for $\kappa$.However, their approximations require (potentially significant) additional computation, and in this short paper we show that given the same amount of computation as their method, one can achieve more accurate approximations using a truncated Newton method. A more interesting contribution of this paper is a simple algorithm for computing Is(x): the modified Bessel function of the first kind. Surprisingly, our na{\"{i}}ve implementation turns out to be several orders ofmagnitude faster for large arguments common to high- dimensional data, than the standard implementations in well-established software such as Mathematica{\textcopyright}, Maple{\textcopyright}, and Gp/Pari.},
author = {Sra, Suvrit},
doi = {10.1007/s00180-011-0232-x},
file = {:home/knub/Repositories/master-thesis/papers/Other/Kappa Estimation.pdf:pdf},
issn = {09434062},
journal = {Computational Statistics},
keywords = {Bessel ratio,Maximum-likelihood,Modified Bessel function,Numerical approximation,von Mises-Fisher distribution},
number = {1},
pages = {177--190},
title = {{A short note on parameter approximation for von Mises-Fisher distributions: And a fast implementation of I s(x)}},
volume = {27},
year = {2012}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Qv and Mikolov, Tomas},
eprint = {1405.4053},
file = {:home/knub/Repositories/master-thesis/papers/Distributed Representaitons of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
issn = {10495258},
journal = {International Conference on Machine Learning - ICML 2014},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Nigam2000,
abstract = {This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled docu- ments. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30{\%}.},
author = {Nigam, Kamal and McCallum, Andrew Kachites and Thrun, Sebastian and Mitchell, Tom},
doi = {10.1023/A:1007692713085},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Text Classification from Labeled and Unlabeled.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Bayesian learning,Expectation-Maximization,combining labeled and unlabeled data,integrating supervised and unsupervised learning,text classification},
number = {2-3},
pages = {103--134},
pmid = {15084654},
title = {{Text Classification from Labeled and Unlabeled Documents using EM}},
url = {http://dl.acm.org/citation.cfm?id=347709.347724},
volume = {39},
year = {2000}
}
@article{Navigli2015,
author = {Navigli, Roberto},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/A Framework for an Intrinsic Evaluation of Word Vector Representations.pdf:pdf},
title = {{Find the word that does not belong : A Framework for an Intrinsic Evaluation of Word Vector Representations}},
year = {2015}
}
@article{Sridhar2015,
abstract = {We present an unsupervised topic model for short texts that performs soft clustering over distributed representations of words. We model the low-dimensional semantic vector space represented by the dense distributed representations of words using Gaussian mixture models (GMMs) whose components capture the notion of latent topics. While conventional topic modeling schemes such as probabilistic latent semantic analysis (pLSA) and latent Dirichlet allocation (LDA) need aggregation of short messages to avoid data sparsity in short documents, our framework works on large amounts of raw short texts (billions of words). In contrast with other topic modeling frameworks that use word co-occurrence statistics, our framework uses a vector space model that overcomes the issue of sparse word co-occurrence patterns. We demonstrate that our framework outperforms LDA on short texts through both subjective and objective evaluation. We also show the utility of our framework in learning topics and classifying short texts on Twitter data for English, Spanish, French, Portuguese and Russian.},
author = {Sridhar, Vivek Kumar Rangarajan},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Unsupervised Topic Modeling for Short Texts Using Distributed
Representations of Words.pdf:pdf},
journal = {Proceedings of NAACL-HLT},
keywords = {short text,topic modeling},
pages = {192--200},
title = {{Unsupervised Topic Modeling for Short Texts Using Distributed Representations of Words}},
year = {2015}
}
@article{Roder2015,
author = {R{\"{o}}der, Michael and Both, Andreas and Hinneburg, Alexander},
doi = {10.1145/2684822.2685324},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Exploring the Space of Topic Coherence Measures.pdf:pdf},
isbn = {9781450333177},
journal = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining - WSDM '15},
keywords = {topic coherence,topic evaluation,topic model},
pages = {399--408},
title = {{Exploring the Space of Topic Coherence Measures}},
url = {http://dl.acm.org/citation.cfm?doid=2684822.2685324},
year = {2015}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/A Neural probabilistic language model.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@article{Reisinger2010,
abstract = {Current vector-space models of lexical semantics create a single "prototype" vector to represent the meaning of a word. However, due to lexical ambiguity, encoding word meaning with a single vector is problematic. This paper presents a method that uses clustering to produce multiple "sense-specific" vectors for each word. This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy. Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models. {\textcopyright} 2010 Association for Computational Linguistics.},
author = {Reisinger, Joseph and Mooney, Raymond J.},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Multi-Prototype Vector-Space Models of Word Meaning.pdf:pdf},
isbn = {1932432655},
journal = {NAACL HLT 2010 - Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference},
number = {June},
pages = {109--117},
title = {{Multi-prototype vector-space models of word meaning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-80053248772{\&}partnerID=tZOtx3y1},
year = {2010}
}
@article{Cheng2015,
author = {Cheng, Jianpeng and Wang, Zhongyuan and Wen, Ji-Rong and Yan, Jun and Chen, Zheng},
doi = {10.1145/2806416.2806517},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Contextual Text Understanding in Distributional Semantic.pdf:pdf},
isbn = {9781450337946},
journal = {Proceedings of the 24th ACM International Conference on Information and Knowledge Management},
keywords = {information retrieval,knowledge representation,machine learn-},
pages = {133--142},
title = {{Contextual Text Understanding in Distributional Semantic Space}},
url = {http://dl.acm.org/citation.cfm?doid=2806416.2806517},
year = {2015}
}
@article{Neelakantan2014,
abstract = {There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a sin-gle vector per word type—ignoring poly-semy and thus jeopardizing their useful-ness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the num-ber of senses per word type, and by its ef-ficiency and scalability. We present new state-of-the-art results in the word similar-ity in context task and demonstrate its scal-ability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Neelakantan, Arvind and Shankar, Jeevan and Passos, Alexandre and Mccallum, Andrew},
eprint = {1504.06654},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space.pdf:pdf},
journal = {Emnlp},
pages = {1059--1069},
pmid = {1687737},
title = {{Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space}},
year = {2014}
}
@article{Rumelhart1988,
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
file = {:home/knub/Repositories/master-thesis/papers/ADA164453.pdf:pdf},
journal = {Cognitive modeling},
pages = {1},
title = {{Learning representations by back-propagating errors}},
volume = {5},
year = {1988}
}
@article{Hinton1986,
abstract = {(From the chapter) first section of this chapter stresses some of the virtues of distributed representations / second section considers the efficiency of distributed representations, and shows clearly why distributed representations can be better than local ones for certain classes of problems / final section discusses some difficult issues which are often avoided by advocates of distributed representations, such as the representation of constituent structure and the sequential focusing of processing effort on different aspects of a structure object (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
annote = {Two theories of representation: distributed vs. undistributed},
author = {Hinton, G E and McClelland, J L and Rumelhart, D E},
doi = {10.1146/annurev-psych-120710-100344},
file = {:home/knub/Repositories/master-thesis/papers/hinton86.pdf:pdf},
isbn = {0-262-68053-X},
issn = {1534-7362},
journal = {Parallel Distributed Processing},
pages = {77--109},
pmid = {21943171},
title = {{Distributed representations}},
year = {1986}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
doi = {10.3115/v1/D14-1162},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/glove.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
pages = {1532--1543},
pmid = {1710995},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@article{Liu2015,
abstract = {Most word embedding models typically represent each word using a single vector, which makes these models indiscriminative for ubiquitous homonymy and polysemy. In order to enhance discriminativeness, we employ latent topic models to assign topics for each word in the text corpus, and learn topical word embeddings (TWE) based on both words and their topics. In this way, contextual word embeddings can be flexibly obtained to measure contextual word similarity. We can also build document representations, which are more expressive than some widely-used document models such as latent topic models. In the experiments, we evaluate the TWE models on two tasks, contextual word similarity and text classification. The experimental results show that our models outperform typical word embedding models including the multi-prototype version on contextual word similarity, and also exceed latent topic models and other representative document models on text classification. The source code of this paper can be obtained from https://github.com/ largelymfs/topical{\_}word{\_}embeddings.`},
author = {Liu, Yang and Liu, Zhiyuan and Chua, Tat-Seng and Sun, Maosong},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Topical Word Embeddings.pdf:pdf},
isbn = {9781577357018},
journal = {Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI'15)},
keywords = {NLP and Text Mining Track},
number = {C},
pages = {2418--2424},
title = {{Topical Word Embeddings}},
volume = {2},
year = {2015}
}
@book{Statistics,
author = {Statistics, Directional},
file = {:home/knub/Repositories/master-thesis/papers/Other/mardia{\&}jupp{\_}2000.pdf:pdf},
isbn = {0471953334},
title = {{No Title}}
}
@article{Wei2006,
abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
author = {Wei, Xing and Croft, W Bruce},
doi = {10.1145/1148170.1148204},
file = {:home/knub/Repositories/master-thesis/papers/LDA/LDA-Based Document Models for Ad-hoc Retrieval.pdf:pdf},
isbn = {1595933697},
issn = {00295515},
journal = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval SIGIR 06},
keywords = {allocation,document model,information retrieval,language model,latent,lda,topic model},
pages = {178},
title = {{LDA-based document models for ad-hoc retrieval}},
volume = {pages},
year = {2006}
}
@article{Pritchard2000,
abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g. , seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home. html.},
author = {Pritchard, Jonathan K. and Stephens, Matthew and Donnelly, Peter},
doi = {10.1111/j.1471-8286.2007.01758.x},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Inference of Population Structure Using Multilocus Genotype Data.pdf:pdf},
isbn = {0016-6731},
issn = {00166731},
journal = {Genetics},
number = {2},
pages = {945--959},
pmid = {10835412},
title = {{Inference of population structure using multilocus genotype data}},
volume = {155},
year = {2000}
}
@misc{Kullback1951,
author = {Kullback, S. and Leibler, R. A.},
file = {:home/knub/Repositories/master-thesis/papers/Other/Kullback{\_}Leibler{\_}1951.pdf:pdf},
publisher = {The annals of mathematical statistics 22.1},
title = {{On Information And Sufficiency}},
year = {1951}
}
@article{LiFei-Fei2005,
abstract = {We propose a novel approach to learn and recognize nat- ural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We repre- sent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previ- ous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes with- out supervision. We report satisfactory categorization per- formances on a large set of 13 categories of complex scenes.},
author = {{Li Fei-Fei} and {Pietro Perona}},
doi = {10.1109/CVPR.2005.16},
file = {:home/knub/Repositories/master-thesis/papers/LDA/A Bayesian Hierarchical Model for Learning Natural Scene Categories.pdf:pdf},
isbn = {0-7695-2372-2},
issn = {10636919},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
keywords = {ayesian hierarchical model for,learning natural scene categories},
pages = {524--531},
title = {{A Bayesian Hierarchical Model for Learning Natural Scene Categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1467486},
volume = {2},
year = {2005}
}
@article{Baroni2010,
abstract = {Computational models of meaning trained on naturally occurring text successfully model human performance on tasks involving simple similarity measures, but they characterize meaning in terms of undifferentiated bags of words or topical dimensions. This has led some to question their psychological plausibility (Murphy, 2002;Schunn, 1999). We present here a fully automatic method for extracting a structured and comprehensive set of concept descriptions directly from an English part-of-speech-tagged corpus. Concepts are characterized by weighted properties, enriched with concept-property types that approximate classical relations such as hypernymy and function. Our model outperforms comparable algorithms in cognitive tasks pertaining not only to concept-internal structures (discovering properties of concepts, grouping properties by property type) but also to inter-concept relations (clustering into superordinates), suggesting the empirical validity of the property-based approach.},
author = {Baroni, Marco and Murphy, Brian and Barbu, Eduard and Poesio, Massimo},
doi = {10.1111/j.1551-6709.2009.01068.x},
file = {:home/knub/Repositories/master-thesis/papers/Datasets/Strudel$\backslash$: A Corpus-Based Semantic Model Based on Properties and Types.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Conceptual knowledge induction,Corpus-based semantic models,Property-based concept representations},
number = {2},
pages = {222--254},
pmid = {21564211},
title = {{Strudel: A corpus-based semantic model based on properties and types}},
volume = {34},
year = {2010}
}
@article{Mikolov2013b,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
annote = {Word Embeddings nicely preserve linear relationships.},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
file = {:home/knub/Repositories/master-thesis/papers/NAACL2013Regularities.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT},
number = {June},
pages = {746--751},
pmid = {1938007},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Linguistic+Regularities+in+Continuous+Space+Word+Representations{\#}0{\%}5Cnhttps://www.aclweb.org/anthology/N/N13/N13-1090.pdf},
volume = {13},
year = {2013}
}
@article{Steyvers2010,
abstract = {Research provides increasing evidence that women and men differ in their decisions to trust. However, information systems research does not satisfactorily explain why these gender differences exist. One possible reason is that, surprisingly, theoretical concepts often do not address the most obvious factor that influences human behavior: biology. Given the essential role of biological factors--and specifically those of the brain--in decisions to trust, the biological influences should naturally include those related to gender. To show empirically that online trust is associated with activity changes in certain brain areas, the authors used functional magnetic resonance imaging (fMRI). In a laboratory experiment, they captured the brain activity of 10 female and 10 male participants simultaneous to decisions on trustworthiness of eBay offers. They found that most of the brain areas that encode trustworthiness differ between women and men. Moreover, they found that women activated more brain areas than did men.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Steyvers, Mark and Griffiths, Tom},
doi = {10.1016/s0364-0213(01)00040-4},
eprint = {1111.6189v1},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Probabilistic Topic Models - SteyversGriffiths.pdf:pdf},
isbn = {1532-4435},
issn = {1386-4564},
journal = {MIS Quarterly},
keywords = {0454,0723,0984,1220,1494,1516,1581 7837,1781 4068 7406 7375 825,2022,2100,2340,2382 7401,2400,2420,2500,2586,2593 677,2598 4068 7406 7375 825,2667,2671 7694,2750,3020,3040,3116 2022,3660,3892 3869 6601 8049 1516,4063 1573,4067 4068 7406 7375 825,4300,454112,4550 8550 1070 5831,4605 1583,4901,4952,5250,5497 7375 825,6213 1516,6242 5831,6613 7375 825,708 5760,7300,7419 5760,7906,8249 6298 7375 825,8390,8444 5760,9130,9190,ATT,Accuracy,Adaptable,Adulthood (18 yrs {\&} older),Aftereffect model,Algorithms,Anomaly detection,Applied,Applied sciences,Astroturf political campaigns,Asymmetric and Private Information (D820),Auctions,Auditory Perception,Automated social engineering,BM25,BNC,BUSINESS ethics,Bayes methods,Bayesian network classifier,Bayesian network classifiers,Behavioral Research,Behavioral sciences,Benchmark testing,Biological Sciences,Bose--Einstein statistics,Botnets,Browsers,Business and Economics,COMMUNITY STRUCTURE,COMPLEX NETWORKS,CONDITIONED response,CONNECTIVITY,CONSUMER education,CONSUMER fraud,CONSUMER protection,Celebrities,Chinese microblog,Classification,Classification algorithms,Clustering,Cognitions,Cognitive Processes,Collective mood,Communication,Communication Systems,Communication and the arts,Communities,Community Networks,Computer Softwa,Computer mediation,Computer science,Computer security,Content spam,Contracts and Reputat,Cooperation,Credibility,Customer relations,Data analysis,Data mining,Data sampling,Data stream,Deception,Decision trees,Detection,Distributed detection,Document clustering,Dyads,Dynamic topic analysis,E-business,EBAY,ELECTRONIC commerce,EMV,Electronic commerce,Electronic mail systems,Emotions,Empirical Study,Employee Attitudes,Employee Interaction,Employees,Employers,Environmental Effects,Environmental Stress,Evolution,Experimental/theoretical,Experimental/theoretical treatment,FALSE advertising,Facebook,Fake reviews,Fake user accounts,Familiarity,Feature extraction,Feature vectors,Feature weighting,Feedback,Female,Filtering,First,Fraud,General Psychology,Genetic algorithms,Germany,Graph centrality,Graph theory,Group {\&} Interpersonal Processes,HIERARCHICAL DIRICHLET PROCESSES,HIGH technology,Heuristic,Hierarchical Dirichlet Process,Human,Humans,INFORMATION resources,INFORMATION resources management,INFORMATION technology,INTERNET advertising,INTERNET marketing,INTERNET users,Image,Immunological,Inference,Information Systems,Information and Internet Services,Information retrieval,Internet,Interpersonal Communication,Interpersonal Influences,Interpersonal Relations,Interpersonal communication,J4.8 algorithm,J48,Katz score,Laplace,Law,LinkedIn,Logistics,Lying,MARKETING,METABOLIC NETWORKS,MODEL,MTIO,Machine Learning,Machine learning,Machine learning algorithms,Male,Malicious users,Malware,Management,Management Personnel,Managers,Mass Media,Mass Media Communications,Models,Monte Carlo simulation,More,Motivation,Natural language processing,Negative selection,New employees,Non-malicious users,Non-parametric topic model,Northern America,ORGANIZATION,Occupational psychology,Okapi BM25,Online Social Networks (OSNs),Online privacy,Online social networks,Ontologies,Ontology,Opinion spam,Organizational Behavior,Organizational behavior,PSYCHOLOGY,Phishing websites,Poisson,Prestige,Probabilistic logic,Problem Solving,Professional Personnel,Professional relationships,Promoter,Proximity,Psychological Theory,Psychological aspects,Psychology: Professional {\&} Research,Public relations,Quantitative Study,RESEARCH,Random variables,Reciprocity,Regression analysis,Reputation,Reputation management,Reputations,Research and Development: Ge,Retail and Wholesale Trade,Retailing,Retailing industry,Review spam,Review spammer detection,SCALE-FREE NETWORKS,SERVICE industries -- Marketing,SMALL-WORLD NETWORKS,SNS features,SVM,Sales,Sales {\&} selling,Second,Semantics,Sentiment analysis,Sentiment tracking,Servers,Service level agreements,Sina Weibo,Social Media,Social Perception,Social Perception {\&} Cognition,Social Sciences,Social Sciences: trends,Social Support,Social aspects,Social conditions {\&} trends,Social media,Social network,Social network analysis,Social network security,Social network service,Social network services,Social networking sites,Social networks,Social research,Social sciences,Social trends {\&} culture,Socialbots,Spam,Spam campaign identification,Spam detection,Spam detection techniques,Spam profile identification,Spamdexing,Spamming,Statistical natural language processing,Stochastic gradient descent,Stock market,Studies,Support vector machines,Survey,Sybil account,Sybil accounts detection,TEXT,TSD,Technological Change,Technology,Telecommunication,Telecommunications systems {\&} Internet communicatio,Text mining,Theory,Topic sentiment analysis,Training,Transactional Relationships,Trust,Trust (Social Behavior),Trust Scale,Truth bias,Twitter,Twitter API,Twitter spam,U 2000,U.S.,URL rate,US,USpam,United States,Unsolicited electronic mail,User Centered Design,User Generated Content,User-generated content,Vectors,Virtual worlds,Visual Perception,WEB,WEB sites,Web 2.0,Web spam,Web spamming prevention techniques,Weibo spam detection method,Willingness to pay,YouTube,[[mkup]]Deception[[mkup]],accounting fraud,ad fraud,ad networks,adaptive deception,adaptive learning paths,affect {\&} cognitions in development of interpersona,affective based trust,amazon,annotations,anomaly detection,ant colony optimization,application,application program interfaces,approximate set similarity,article,artificial neural network,auction,automated Turing tests,automated deception detection,bayesian,bayesian parameter estimation,belief networks,belief propagation,binomial law,bipartite cores,blogs,book,bookitem,card not present,classification,classification method,click spam detection,cliques enumeration,cmc,coalition fraud attacks,coevolutionary algorithms,cognitive based trust,communication technology,computational model,computer network security,consistency of visible {\&} audible characteristics,content-based spam detection frameworks,cooperative behavior,credibility,credit card,crowdsourced manipulation,crowdsourcing,crowdsourcing site,data mining,data sampling,dblp,deceptive review,deep belief network,defection,detecting deception,detection,discrepancy-arousal hypothesis,distrust,document length normalization,e-Commerce (L810),e-business,e-commerce Web sites,e-mail,e-markets,eBay Inc,ecology,effective features,electronic markets,eliteness,email,emerging outbreak monitoring,emnlp2007,environmental psychology,evaluation,expertise location,expressive behavio,facebook,fake identities,fake product reviews,fake reviews,fake web review,feature extraction,file-import-08-07-28,financial fraud,framework,fraud,fraud detection,fraud evidence,fraud prevention,fuzzy logic,graph theory,graphical model,heterogenous networks,iSRD,identity,idf,imbalanced data distribution,imbalanced data distributions,information,information retrieval,information systems,interaction rate,internet advertising,invasive software,large margin classifiers,latent dirichlet allocation,lda,learning (artificial intelligence),learning methods,least-squares,logistic regression,long-surviving Twitter spam accounts,machine learning algorithm,machine learning classifiers,malware,markov random fields,media richness,mercer,meta-memory,metadata,microblog,microblogging social network,microblogging spam detection,multi-instance learning,multiple motives of deception as strategic communi,naive Bayes,on-line intrusion analysis,online advertising,online communities,online customer service,online fraud,online learning,online markets,online shopping process,ontologies (artificial intelligence),opinion mining,opinion spam,opinion spam detection,outlier detection,parisian approach,pattern classification,pattern discovery,penalty distance,perceptrons,plagiarism,plda,post,privacy,probabilistic models,product reviews,promoter,pvnets,randomness,real data experiments,regression,regularity,reputation management,reputation mechanisms,reputation systems,resource,response,retail,retail data processing,retreival,review analysis,review helpfulness,review site,review spam,review spammer,review utility,reviewer behavior,reward distance,scalable algorithms,security,self-nonself space pattern construction,self-similarity,semi-supervised learning,sentiment,sentiment analysis,service quality,shilling,signaling,similarity-sensitive sampling,sir,smoothing,social honeypots,social influence,social media,social media spam,social network security,social networking (online),social networking Websites,social networking services,social networks,social spam,social spam discovery,social-networks,social-spam,spam,spam detection,spam message detection,spam review detection,spammer,spammer identification,spamming behavior,spectrum,splog detection,stepwise,succession law,supervised classification methods,support vector machine,support vector machines,supporting services,supporting services functionality,sweethearting,tag,tag similarity,teiresias,temporal dynamics,term frequency normalization,term weighting,text categorization,text classification,topic model,topic modeling,topology,transactive memory,trust,trust management,trust propagation,trusted computing,twitter,unexpected patterns,unsolicited e-mail,user accounts,user centric ontology,user-generated content,video,video promotion,video response,video spam,web 2.0,web forum,web of trust,web security,web spam detection,wide scale proliferation},
number = {3},
pages = {993--1022},
pmid = {21362469},
title = {{Probabilistic Topic Models}},
url = {http://www.sciencedirect.com/science/article/pii/S0140366413001047{\%}5Cnhttp://ceas.cc/2004/167.pdf{\%}5Cnhttp://doi.acm.org/10.1145/1806338.1806450{\%}5Cnhttp://eprints.soton.ac.uk/272254/{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7033160{\%}5Cnhttp://},
volume = {3},
year = {2010}
}
@misc{Rennie2008,
author = {Rennie, Jason},
title = {{The 20 Newsgroups data set}},
url = {http://qwone.com/{~}jason/20Newsgroups/},
urldate = {2016-11-22},
year = {2008}
}
@article{Deerwester1990,
abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents ("semantic structure") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising.},
archivePrefix = {arXiv},
arxivId = {arXiv:1403.2923v1},
author = {Deerwester, Scott and Dumais, Susan T. and Harshman, Richard},
doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
eprint = {arXiv:1403.2923v1},
file = {:home/knub/Repositories/master-thesis/papers/Indexing by Latent Semantic Analysis.pdf:pdf},
isbn = {9781450300322},
issn = {0002-8231},
journal = {Journal of the American society for information science},
number = {6},
pages = {391--407},
pmid = {470195},
title = {{Indexing by latent semantic analysis}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Indexing+by+Latent+Semantic+Analysis{\#}0},
volume = {41},
year = {1990}
}
@inproceedings{Rehurek2010,
author = {Rehurek, Radim and Sojka, Petr},
booktitle = {Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks},
pages = {45--50},
publisher = {ELRA},
title = {{Software Framework for Topic Modelling with Large Corpora}},
year = {2010}
}
@inproceedings{Wang2009,
abstract = {This paper presents PLDA, our parallel implementation of Latent Dirichlet Allocation on MPI and MapReduce. PLDA smooths out storage and computation bottlenecks and provides fault recovery for lengthy distributed computations. We show that PLDA can be applied to large, real-world applications and achieves good scalability. We have released MPI-PLDA to open source at http://code.google.com/p/plda under the Apache License.},
author = {Wang, Yi and Bai, Hongjie and Stanton, Matt and Chen, Wen Yen and Chang, Edward Y.},
booktitle = {International Conference on Algorithmic Applications in Management},
doi = {10.1007/978-3-642-02158-9_26},
file = {:home/knub/Repositories/master-thesis/papers/LDA/PLDA$\backslash$: Parallel Latent Dirichlet Allocation.pdf:pdf},
isbn = {3642021573},
issn = {03029743},
pages = {301--314},
publisher = {Springer},
title = {{PLDA: Parallel latent dirichlet allocation for large-scale applications}},
year = {2009}
}
@article{Krestel2012,
author = {Krestel, Ralf},
file = {:home/knub/Repositories/master-thesis/papers/Use of Language and Topic Models on the Web - Krestel.pdf:pdf},
title = {{On the Use of Language Models and Topic Models in the Web}},
url = {http://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/?{\_}r=0},
year = {2012}
}
@article{Schwarz1978,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
file = {:home/knub/Repositories/master-thesis/papers/Other/Estimating the Dimension of a Model.pdf:pdf},
isbn = {0780394224},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
pages = {461--464},
pmid = {2958889},
title = {{Estimating the dimension of a model}},
url = {http://projecteuclid.org/euclid.aos/1176344136},
volume = {6},
year = {1978}
}
@article{Baroni2014,
abstract = {Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts.},
author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, German},
doi = {10.3115/v1/P14-1023},
file = {:home/knub/Repositories/master-thesis/papers/Don't count, predict! A systematic comparison of context-counting vs context-predicting semantic vectors.pdf:pdf},
isbn = {9781937284725},
journal = {ACL},
pages = {238--247},
title = {{Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors}},
year = {2014}
}
@article{Niu2015,
abstract = {Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than LDA-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.},
archivePrefix = {arXiv},
arxivId = {1506.08422},
author = {Niu, Li-Qiang and Dai, Xin-Yu},
eprint = {1506.08422},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Topic2Vec Learning Distributed Representations of Topics.pdf:pdf},
journal = {arXiv:1506.08422 [cs]},
title = {{Topic2Vec: Learning Distributed Representations of Topics}},
url = {http://arxiv.org/abs/1506.08422{\%}5Cnhttp://www.arxiv.org/pdf/1506.08422.pdf},
year = {2015}
}
@article{Nguyen2015,
abstract = {Probabilistic topic models are widely used to discover latent topics in document collec- tions, while latent feature vector representa- tions of words have been used to obtain high performance in many NLP tasks. In this pa- per, we extend two different Dirichlet multino- mial topic models by incorporating latent fea- ture vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus. Exper- imental results show that by using informa- tion from the external corpora, our new mod- els produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.},
author = {Nguyen, Dat Quoc and Billingsley, Richard and Du, Lan and Johnson, Mark},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Improving Topic Models with Latent Feature Word Representations.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {299--313},
title = {{Improving topic models with latent feature word representations}},
volume = {3},
year = {2015}
}
@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
annote = {Introducing CBOW and Skip-gram models for computing word embeddings.
https://code.google.com/archive/p/word2vec/

* LDA can also provide word vectors by looking at the topics for a single word, however: "distributed representations perform signifcantly better for preserving linear regularities"
* Also, LDA is very expensive on large data sets
* EVALUATION: Using question pairs, e.g. Paris-France, Berlin-???
* DATASET: Microsoft Sentence Completion Challenge
* EVALUATION: idea: catching out of the list words, e.g. Paris, Berlin, Warsaw, London, Broccoli
* DATASET: SemEval-2012 Task 2: Determine similarity of word pairs A-B and C-D, e.g. dog:bark, cat:meow
* USECASES WE: sentiment analysis, paraphrase detection},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Efficient Estimation of Word Representations in Vector Space - Mikolov et al.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@article{Newman2009,
abstract = {We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straightforward mapping of LDA to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled LDA. The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both LDA and HDP, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.},
author = {Newman, David and Welling, Max},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Distributed Algorithms for Topic Models.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {distributed,hierarchical dirichlet processes,latent dirichlet allocation,parallel computation,topic models},
pages = {1801--1828},
title = {{Distributed Algorithms for Topic Models}},
url = {http://portal.acm.org/citation.cfm?id=1755845},
volume = {10},
year = {2009}
}
@article{Goth2016,
abstract = {MARCH 2016 | VOL. 59 | NO. 3 | COMMUNICATIONS OF THE ACM 13 news N O NE OF THE featured speak-ers at the inaugural Text By The Bay conference, held in San Francisco in April 2015, drew laughter when describing a neural network question-answering model that could beat hu-man players in a trivia game. While such performance by com-puters is fairly well known to the general public, thanks to IBM's Wat-son cognitive computer, the speaker, natural language processing (NLP) researcher Richard Socher, said, the neural network model he described " was built by one grad student using deep learning " rather than by a large team with the resources of a global corporation behind them. Socher, now CEO of machine learn-ing developer MetaMind, did not in-tend his remarks to be construed as a comparison of Watson to the academic model he and his colleagues built. As an illustration of the new technical and cultural landscape around NLP, how-ever, the laughter Socher's comment drew was an acknowledgment that ba-sic and applied research in language processing is no longer the exclusive province of those with either deep pockets or strictly academic intentions.},
author = {Goth, Gregory},
doi = {10.1145/2874915},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/NLP Is Breaking Out.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {3},
pages = {13--16},
title = {{Deep or shallow, NLP is breaking out}},
url = {http://dl.acm.org/citation.cfm?doid=2897191.2874915},
volume = {59},
year = {2016}
}
@article{Schnabel2015,
abstract = {We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations ofwords from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.},
author = {Schnabel, Tobias and Labutov, Igor and Mimno, David and Joachims, Thorsten},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Evaluation methods for unsupervised word embeddings.pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
keywords = {Distributional semantics,Evaluation techniques},
number = {September},
pages = {298--307},
title = {{Evaluation methods for unsupervised word embeddings}},
year = {2015}
}
@inproceedings{Maas2011,
abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term–document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment in- formation as well as non-sentiment annota- tions. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
author = {Maas, Andrew L and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
doi = {978-1-932432-87-9},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Learning Word Vectors for Sentiment Analysis.pdf:pdf},
isbn = {9781932432879},
pages = {142--150},
title = {{Learning Word Vectors for Sentiment Analysis}},
year = {2011}
}
@article{Chang2009,
author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, J. L. and Blei, David M},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Reading tea leaves.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 22},
pages = {288--296},
title = {{Reading Tea Leaves : How Humans Interpret Topic Models}},
year = {2009}
}
@book{Jordan2006,
abstract = {Methods of dimensionality reduction provide a way to understand and visualize the structure of complex data sets. Traditional methods like principal component analysis and classical metric multidimensional scaling suffer from being based on linear models. Until recently, very few methods were able to reduce the data dimensionality in a nonlinear way. However, since the late nineties, many new methods have been developed and nonlinear dimensionality reduction, also called manifold learning, has become a hot topic. New advances that account for this rapid growth are, e.g. the use of graphs to represent the manifold topology, and the use of new metrics like the geodesic distance. In addition, new optimization schemes, based on kernel techniques and spectral decomposition, have lead to spectral embedding, which encompasses many of the recently developed methods. This book describes existing and advanced methods to reduce the dimensionality of numerical databases. For each method, the description starts from intuitive ideas, develops the necessary mathematical details, and ends by outlining the algorithmic implementation. Methods are compared with each other with the help of different illustrative examples. The purpose of the book is to summarize clear facts and ideas about well-known methods as well as recent developments in the topic of nonlinear dimensionality reduction. With this goal in mind, methods are all described from a unifying point of view, in order to highlight their respective strengths and shortcomings. The book is primarily intended for statisticians, computer scientists and data analysts. It is also accessible to other practitioners having a basic background in statistics and/or computational learning, like psychologists (in psychometry) and economists.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher},
booktitle = {Pattern Recognition},
doi = {10.1641/B580519},
eprint = {0-387-31073-8},
file = {:home/knub/Repositories/master-thesis/papers/Bishop - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {9780387310732},
issn = {00063568},
number = {356},
pages = {791--799},
pmid = {18292226},
title = {{Information Science and Statistics}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Aletras2013,
author = {Aletras, Nikolaos and Stevenson, Mark},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Evaluating Topic Coherence Using Distributional Semantics.pdf:pdf},
journal = {Proc. of the 10th Int. Conf. on Computational Semantics (IWCS '13)},
number = {October 2016},
pages = {13--22},
title = {{Evaluating Topic Coherence Using Distributional Semantics}},
year = {2013}
}
@article{Levy2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
doi = {10.1186/1472-6947-15-S2-S2},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Improving Distributional Similarity.pdf:pdf},
isbn = {1472-6947 (Electronic)$\backslash$r1472-6947 (Linking)},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
pmid = {26099735},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
volume = {3},
year = {2015}
}
@article{SparckJones1972,
abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently‐occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
author = {{Sparck Jones}, Karen},
doi = {10.1108/eb026526},
file = {:home/knub/Repositories/master-thesis/papers/LDA/A STATISTICAL INTERPRETATION OF TERM SPECIFITY.pdf:pdf},
issn = {0022-0418},
journal = {Journal of Documentation},
number = {1},
pages = {11--21},
title = {{A Statistical Interpretation of Term Specificity and Its Application in Retrieval}},
url = {http://www.emeraldinsight.com/doi/abs/10.1108/eb026526},
volume = {28},
year = {1972}
}
@article{Newman2010,
abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on point- wise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
author = {Newman, David and Lau, Jh and Grieser, Karl and Baldwin, Timothy},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Automatic Evaluation of Topic Coherence.pdf:pdf},
isbn = {1932432655},
journal = {{\ldots} Language Technologies: The {\ldots}},
number = {June},
pages = {100--108},
title = {{Automatic evaluation of topic coherence}},
url = {http://dl.acm.org/citation.cfm?id=1858011},
year = {2010}
}
@article{Zou2013,
abstract = {We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.},
author = {Zou, Will Y and Socher, Richard and Cer, Daniel and Manning, Christopher D},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/Bilingual Word Embeddings for Phrase-Based Machine Translation.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)},
pages = {1393--1398},
title = {{Bilingual Word Embeddings for Phrase-Based Machine Translation}},
year = {2013}
}
@misc{Das2016,
author = {Das, Rajarshi},
publisher = {Personal communication},
title = {{Personal communication}},
year = {2016}
}
@article{Agirre2009,
abstract = {This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.},
author = {Agirre, Eneko and Alfonseca, Enrique and Hall, Keith and Kravalova, Jana and Pas, Marius and Soroa, Aitor},
doi = {10.3115/1620754.1620758},
file = {:home/knub/Repositories/master-thesis/papers/Other/A study on similarity and relatedness using distributional and WordNet-based approaches.pdf:pdf},
isbn = {978-1-932432-41-1},
issn = {1351-3249},
journal = {Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL},
number = {June},
pages = {19--27},
title = {{A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches}},
url = {http://www.aclweb.org/anthology/N09-1003},
year = {2009}
}
@article{Snoek2012,
abstract = {超パラメータのベイズ最適化},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Rp},
doi = {2012arXiv1206.2944S},
eprint = {1206.2944},
file = {:home/knub/Repositories/master-thesis/papers/Other/Practical Bayesian Optimization of Machine Learning Algorithms.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms.}},
url = {https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
year = {2012}
}
@article{VanDerMaaten2008,
abstract = {KNAW Narcis. Back to search results. Publication - - (2008). Pagina-navigatie: Main. Title, - - . Published in, Journal of Machine Learning Research, Vol. 9, No. nov, p.2579-2605.},
author = {{Van Der Maaten}, L J P and Hinton, G E},
doi = {10.1007/s10479-011-0841-3},
file = {:home/knub/Repositories/master-thesis/papers/Other/Visualizing High-Dimensional Data Using t-SNE.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing high-dimensional data using t-SNE}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=7911431479148734548related:VOiAgwMNy20J},
volume = {9},
year = {2008}
}
@misc{Mikolov2013d,
author = {Mikolov, Tomas},
pages = {https://groups.google.com/forum/{\#}!msg/word2vec--too},
title = {{Tomas Mikolov on preprocessing for word2vec}},
year = {2013}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
annote = {THE standard LDA paper},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Latent Dirichlet Allocation - Blei Ng Jordan.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {lda,topic model},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
url = {http://www.cs.princeton.edu/{~}blei/lda-c/{\%}5Cnpapers2://publication/doi/10.1162/jmlr.2003.3.4-5.993{\%}5Cnpapers2://publication/uuid/4001D0D9-4F9C-4D8F-AE49-46ED6A224F4A{\%}5Cnpapers2://publication/uuid/7D10D5DA-B421-4D94-A3ED-028107B7F9B6{\%}5Cnhttp://www.crossref.org/jmlr},
volume = {3},
year = {2003}
}
@article{Rosner2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1403.6397v1},
author = {Rosner, F},
eprint = {arXiv:1403.6397v1},
file = {:home/knub/Repositories/master-thesis/papers/LDA/Evaluating Topic Coherence Measures.pdf:pdf},
journal = {CoRR},
pages = {1--4},
title = {{Evaluating topic coherence measures}},
year = {2013}
}
@article{Li2016,
abstract = {Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document.},
archivePrefix = {arXiv},
arxivId = {1606.02979},
author = {Li, Shaohua and Chua, Tat-Seng and Zhu, Jun and Miao, Chunyan},
eprint = {1606.02979},
file = {:home/knub/Repositories/master-thesis/papers/LDA vs. or + Word Embeddings/Generative Topic Embedding.pdf:pdf},
title = {{Generative Topic Embedding: a Continuous Representation of Documents (Extended Version with Proofs)}},
url = {http://arxiv.org/abs/1606.02979},
year = {2016}
}
@misc{Gilks1996,
abstract = {convince readers it is simple and has potential},
author = {Gilks, Walter R. and Richardson, Sylvia and Spiegelhalter, David J.},
booktitle = {Markov Chain Monte Carlo in Practice},
doi = {10.1007/978-1-4899-4485-6_1},
file = {:home/knub/Repositories/master-thesis/papers/Other/MCMC1.pdf:pdf},
isbn = {0412055511},
issn = {0849-6757},
pages = {512},
title = {{Introducing Markov Chain Monte Carlo}},
url = {http://link.springer.com/chapter/10.1007/978-1-4899-4485-6{\_}1},
year = {1996}
}
@article{Zweig2011,
abstract = {Work on modeling semantics in text is progressing quickly, yet there are few existing public datasets which authors can use to measure and compare their systems. This work takes a step towards addressing this issue. We present the MSR Sentence Completion Challenge Data, which consists of 1,040 sentences, each of which has four impostor sentences, in which a single (fixed) word in the original sentence has been replaced by an impostor word with similar occurrence statistics. For each sentence the task is then to determine which of the five choices for that word is the correct one. This data was constructed from Project Gutenberg data. Seed sentences were selected from Sherlock Holmes novels, and then imposter words were suggested with the aid of a language model trained on over 500 19th century novels. The language model was used to compute 30 alternative words for a given low frequency word in a sentence, and human judges then picked the 4 best impostor words, based on a set of provided guidelines. Although the data presented here will not be changed, this is still a work in progress, and we plan to add similar datasets based on other sources. This technical report is a living document and will be updated appropriately as new datasets are constructed and new results on existing datasets (for example, using human subjects) are reported.},
annote = {Use as DATASET for EVALUATION?
Nice application for word2vec, also LSA is used in the Paper itself, so it might also work for LDA.
Open QUESTION: How to use LDA on only one sentence?},
author = {Zweig, Geoffry and Burges, Christopher J C},
file = {:home/knub/Repositories/master-thesis/papers/Datasets/Microsoft Sentence Completion Challenge.pdf:pdf},
journal = {Challenge},
pages = {1--7},
title = {{The Microsoft Research Sentence Completion Challenge}},
year = {2011}
}
@article{Levy2014a,
abstract = {Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.'s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.},
author = {Levy, Omer and Goldberg, Yoav},
file = {:home/knub/Repositories/master-thesis/papers/Word Embeddings/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf:pdf},
isbn = {9781941643020},
journal = {CoNLL},
pages = {171--180},
title = {{Linguistic regularities in sparse and explicit word representations}},
url = {http://anthology.aclweb.org/W/W14/W14-16.pdf{\#}page=181},
year = {2014}
}
