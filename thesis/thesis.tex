\documentclass[
        a4paper,
        titlepage,
        twoside,
        parskip
        ]{scrbook}
\setlength{\parskip}{6pt}
\setlength{\parindent}{15pt}

\usepackage[table,usenames]{xcolor}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{darkgreen}{rgb}{0,0.4,0}
\definecolor{darkblue}{rgb}{0,0,0.5}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
% \usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\specialcell}[2][c]{%
         \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\usepackage{textcomp,amsmath}     % Mathezeichen etc.
\usepackage{graphicx}             % Graphiken einbinden
\usepackage{makecell}
\usepackage{soul}
\graphicspath{{./figures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage{rotating}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{threeparttable}

% bibtex
\usepackage{url}
\usepackage[backend=bibtex, style=numeric]{biblatex}

\bibliography{library}

\usepackage{paralist}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{listings}
\usepackage[toc,page]{appendix}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  citecolor=darkgreen,
  linkcolor=darkred,
  urlcolor=darkblue,
  filecolor=red,
  pdftitle={Master Thesis},
  pdfauthor={Stefan Bunk},
  pdfpagemode={UseNone}
}
\usepackage{cleveref}
\usepackage{varwidth}
\DeclareCaptionFormat{centerformat}{%
  % #1: label (e.g. "Table 1")
  % #2: separator (e.g. ": ")
  % #3: caption text
  \begin{varwidth}{\linewidth}%
    \centering
    #1#2#3%
  \end{varwidth}%
}
\captionsetup{format=centerformat}% global activation

\renewcommand\theadfont{\bfseries}
\newcommand*{\tsubhead}[1]{\itshape #1}

\usepackage{amsthm}
\newtheorem{keyexmp}{Keyword Example}[section]

\newtheoremstyle{break}%
{}{}%
{\itshape}{}%
{\bfseries}{}% % Note that final punctuation is omitted.
{\newline}{}
\theoremstyle{break}
\newtheorem{exmp}{Example}[section]
\crefname{exmp}{Example}{Examples}

%\newenvironment{fexmp}
%  {\begin{mdframed}\begin{exmp}}
%  {\end{exmp}\end{mdframed}}

% Make a | to a | surrounded by spaces
\newcommand{\spacedpipe}{\ |\ }
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\titlehead{
\includegraphics{figures/hpi_logo_cmyk_wb_sl2}
} \subject{Masterarbeit}
\title{Improving Probabilistic Topic Models using Word Embeddings
\\ \bigskip
\large{German Title}}
\author{Stefan Bunk\\{\small{\url{stefan.bunk@student.hpi.uni-potsdam.de}}}}
\date{Eingereicht am 23.12.2016}
\publishers{
Fachgebiet Informationssysteme \\
Betreuung: Dr. Ralf Krestel}

\pagestyle{headings}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{document}
% \frontmatter
\maketitle
\cleardoublepage

\include{abstract}
\tableofcontents
% \mainmatter

\chapter{Introduction}
% \begin{algorithm}
%   \caption{Gibbs sampling in LDA}
%   \label{alg:welda_init}
%   \begin{algorithmic}[1]
%     \Procedure{GibbsSampling}{set of documents}
%     \State{Randomly initialize words to topics}
%     \For{iteration i}
%       \For{document d}
%         \For{word w in document d}
%           \State{Calculate distribution over topics for word w}
%           \State{Sample new topic for word w}
%           \State{Update counts}
%         \EndFor
%       \EndFor
%     \EndFor
%     \EndProcedure
%   \end{algorithmic}
% \end{algorithm}
% \begin{equation*}
%   p(z_w = t | Z) \propto (N_{d}^{t} + \alpha) \times  \frac{N^{t, w} + \beta}{N^{t} + V * \beta}
% \end{equation*}
% \begin{algorithm}
%   \caption{Gibbs sampling in WELDA}
%   \label{alg:welda_init}
%   \begin{algorithmic}[1]
%     \Procedure{GibbsSamplingWELDA}{set of documents}
%     \State{Run LDA as initialization step}
%     \For{iteration i}
%       \State{Estimate distribution parameters for all topics independently}
%       \For{document d}
%         \For{word w in document d}
%           \State{Calculate distribution over topics for word w}
%           \State{new\_topic\_id $\gets$ Sample new topic for word w}
%           \If{coin\_flip($\lambda$)}
%             \State{sample $\gets$ Sample from distribution of $new\_topic\_id$}
%             \State{w $\gets$ Find nearest word for $sample$}
%           \EndIf
%           \State{Update counts}
%         \EndFor
%       \EndFor
%     \EndFor
%     \EndProcedure
%   \end{algorithmic}
% \end{algorithm}

A core task in natural language processing (NLP) is to understand the meaning of words.
Many downstream NLP tasks benefit from this, for example, text categorization, part-of-speech tagging, and machine translation.
A popular concept to qualify the meaning of a word is to look at the contexts in which the word appears.
A famous quote by Firth says: ``You shall know a word by the company it keeps''~\cite{Firth1957}.
This assumption is also known as the distributional hypothesis.

Two existing approaches in this area are topic modeling and word embeddings.
Topic modeling assumes that an author has certain topics in mind when writing a document, which are chosen beforehand.
For example, an author could decide to write a document about \emph{politics} (70~\%) in \emph{sports} (30~\%) and then picks the words ``coalition'', ``election'' and ``corruption'' for \emph{politics} and the words ``soccer'' and ``ball'' for \emph{sports}.
Topic modeling techniques, with latent Dirichlet allocation being the most popular one, try to recover the hidden topics in large corpora, i.e.\ find out that ``soccer'' and ``ball'' come from the same topic.
As topic models are probabilistic models and hence provide probability distributions as the result, the topics can be easily interpreted by humans.

With word embeddings each word is assigned a vector in a high-dimensional vector space.
These vectors are automatically learned using neural networks.
During the supervised training each word must try to predict its surrounding words.
This way, the context of a word is encoded in the vector representation of a word.
Using a clever network architecture, these vectors exhibit interesting properties.
First, similar words tend to be in similar positions in the vector space.
For example, when looking for the most similar words for ``France'' using cosine distance, the model outputs ``Spain'', ``Belgium'', ``Netherlands'' and ``Italy''.
Second, the word vectors exhibit interesting linear relationships.
For example, when calculating the vector $vector("King") - vector("Man") + vector("Woman")$ and looking for the closest word at the vector result, the result is the word ``queen''~\cite{Mikolov2013b}.

The two approaches have different origins.
Word embeddings have their roots in the neural network and deep learning community, while topic modeling stems and Bayesian statistics.
% https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/ THIS BLOG POST IS VERY ENLIGHTENING, GIVES A GOOD OVERVIEW
% These different contextual representations capture different types of semantic similarity; the document-based models capture semantic relatedness (e.g. “boat” – “water”) while the word-based models capture semantic similarity (e.g. “boat” – “ship”). This very basic difference is too often misunderstood.
Hence, there is relatively little research in combination of these two methods.
In this master thesis, we aim to explore potential synergies between these technologies.
In particular, it might be useful to investigate ``whether the two types of models are complementary in the errors they make, in which case combined models could be an interesting avenue for future work''~\cite{Baroni2014}.

This master thesis is organized as follows: Chapter~\ref{sec:related-work} will introduce topic modeling and word embeddings, compare the two approaches and show existing work in combining the two methods.
Chapter~\ref{sec:comparative-evaluation} will present first experiments, which we want to conduct at the start of this master thesis.
Chapter~\ref{sec:approach} will show our approach for this master thesis.
% Section~\ref{sec:data-sets-and-evaluation} will show different data sets and evaluation tasks to evaluate systems based on topic modelling and word embeddings.

\chapter{Related work}
\label{chapter:related-work}
\section{Topic modeling}

Many methods for modeling natural language text have been proposed.
An early popular method was the tf-idf~\cite{SparckJones1972} scheme, which reduced each document in a corpus to a vector of term frequencies normalized by inverse document frequencies.
Later, latent semantic analysis (LSA)~\cite{Deerwester1990} was proposed to reduce the large matrix to a smaller subspace using singular value decomposition.
While these approaches have their origins in linear algebra, topic models have been developed from a probabilistic view to allow better interpretation.
Topic models are generative probabilistic models of a document collection, which assume hidden topics have guided the generation of the text.
In this way of thinking, an author picks certain topics to write about, and then picks words associated with these topics for a specific document.
The goal is then to uncover the unseen, so called ``latent'', topics from a text corpus.
All of these approaches have in common that they assume a bag-of-words representation of documents.

\subsection{Early approaches}
A basic generative topic model is the mixture of unigrams model~\cite{Nigam2000}.
In this model, a topic is a fixed probability distribution over the vocabulary.
However, only one topic is allowed per document, which limits its predictive power.

A more sophisticated topic model is probabilistic latent semantic analysis (pLSA), which allows multiple topics per document.
The model is fitted with an expectation-maximization algorithm.
However, pLSA does not provide a full generative model applicable to unseen documents.
Also, as the number of parameters grows linearly with the number of training documents, it tends to overfit~\cite{Blei2003}.

\subsection{Latent Dirichlet allocation (LDA)}
LDA was proposed by Blei et al.~\cite{Blei2003} for modeling text corpora and other collections of discrete data.
In an unsupervised fashion, it can automatically detect similarities between words and group them in one topic.
The number $K$ of topics is constant and must be chosen beforehand.
% TODO thesis: Add nice example document with figure, e.g. k=3 and some topics

LDA defines a topic as a distribution over a fixed vocabulary.
Different topics assign different probabilities to the same word, for example, the word ``soccer'' would have a much higher probability in a \emph{sports} topic than in a \emph{politics} topics.
The opposite would hold for the word ``coalition''.
Note that a topic is only a distribution over words, i.e.\ LDA does not generate a name or a summary for a topic.

A document is assumed to consist of one or several topics with a certain, fixed distribution.
A document could be $20~\%$ about \emph{sports} and $80~\%$ percents about \emph{politics}, or $30~\%$ about \emph{climate}, $25~\%$ about \emph{cars} and $45~\%$ about \emph{politics}.
In general, LDA aims to generate sparse distributions for both cases, i.e.\ a topic should have only a few words (relative to the vocabulary of the entire corpus) with high probability and a document should have only a few topics.
This is governed by two scalar hyperparameters $\alpha$ and $\beta$, which influence two symmetrical prior Dirichlet distributions.

With these two hyperparameters, the generative document creation process of LDA can be summarized as follows:
\begin{enumerate}
       \item Fix number of topics $K$ in the document collection
       \item Choose word distributions $\phi_j \sim Dirichlet(\beta)$ for each topic $j \in \{1~..~K\}$
       \item Now, for each document $d$:
       \begin{enumerate}
              \item Choose topic distribution $\theta_d \sim Dirichlet(\alpha)$
              \item For each position $i$ in the document
              \begin{enumerate}
                     \item Choose topic $z_{d,i} \sim Multinomial(\theta_d)$
                     \item Choose word $w \sim Multinomial(\phi_{z_{d,i}})$
              \end{enumerate}
       \end{enumerate}
\end{enumerate}
This process is also illustrated in the probabilistic graphical model in plate notation in Figure \ref{fig:lda}.
This model can also be seen as a soft-classification of documents: instead of assuming exactly one topic per document, each word in a document can potentially come from a different topic.

\begin{figure}
       \centering
       \includegraphics{figures/lda.pdf}
       \caption{Graphical model for latent Dirichlet allocation with $K$ topics, $M$ documents and $N$ words in a document.}
       \label{fig:lda}
\end{figure}

In reality, we only observe the words in the documents, and need to reverse engineer the hidden, ``latent'' parameters of the model, namely the word distribution per topic $\phi$, the topic distribution per document $\theta$ and the word-topic assignments $z$.
In this process, called \emph{inference}, we try to find the most probable parameter assignments, which could have generated our document collection.
Computationally, this is equivalent to finding the posterior distribution of the parameters given the observed words.
Blei et al.~\cite{Blei2003} propose an algorithm based on variational approximation, however Griffiths and Steyvers~\cite{Griffiths2004} proposed an algorithm based on Gibbs sampling. %, which is more popular nowadays. -- reference missing
% expand here: limiting distribution, sampling formula etc.

The output of running LDA on a corpus is three-fold:
\begin{itemize}
       \item Topics as set of similar words, e.g.\ the terms ``soccer'', ``ball'', ``league'' could form a topic named \emph{sports}.
       \item Topic distribution of a single word, e.g.\ the term ``jaguar'' occurs 80~\% in the \emph{cars} topic and 20~\% in the \emph{animals} topic
       \item Topic distribution inside a specific document or inside the entire corpus
\end{itemize}

LDA has been applied to many domains, namely document modeling, text classification, collaborative filtering~\cite{Blei2003} and document summarization~\cite{Wang2009}.
Other use cases include population genetics~\cite{Pritchard2000} and computer vision~\cite{LiFei-Fei2005}.

\paragraph{Implementations}
There exist implementations of LDA in Java, C/C++, Python and Matlab.
In this thesis, we will use two particularly fast and robust implementations:
\begin{itemize}
       \item Mallet by McCallum~\cite{McCallum2002}, Java
       \item gensim by {\v R}eh{\r u}{\v r}ek~\cite{Rehurek2010}, Python
\end{itemize}

% LDA has problems on small texts/documents


\subsection{Evaluation of topic models}
As topic modeling is an unsupervised learning technique, there is no straightforward evaluation metric available as in supervised tasks.
The output of a topic model, the document-topic matrix and the topic-word matrix, do not lend themselves to a quantitative evaluation either.
Still, with many different topic modeling techniques available and hyperparameters to set, there is a need to compare topic models and to be able to choose the best topic.

Traditionally, topic models were evaluated using held-out likelihood of unseen documents.
In this approach, a topic model was trained on a corpus.
Afterwards, the likelihood of some held-out documents was calculated using the trained model.
The intuition was, that the higher this likelihood, the better the topic model.
However, Chang et al.~\cite{Chang2009} showed, that this held-out likelihood does not correlate with the interpretability for humans very well.
Thus, another topic model evaluation measure was necessary.

The idea of \emph{topic coherence} is to measure topic models based on the top words of all of its topics.
This corresponds to the way, that humans interact with topic models mostly: by looking at the top words in a topic and mentally assigning an umbrella term to a topic.
Thus, topic coherence measures, how well the top-words in the topics belong together and form a common theme.
A good topic (examples taken from \cite{Boyd-graber2014}) is: \\
\hspace*{1cm} \framebox[1.1\width]{trout fish fly fishing water angler stream rod flies salmon} \\
The words in topic clearly form a narrow topic, all words belong to the common ``fishing'' theme.
One can easily imagine a document, which contains all of these words.
On the contrary, an example for a bad topic is: \\
\hspace*{1cm} \framebox[1.1\width]{notion sense choice situation idea natural explictly explicit definition refer} \\
The topic contains very general words, no clear common theme is recognizable.
This topic is not useful for a human trying to understand a document collection.

Boyd-Graber et al.~\cite{Boyd-graber2014} et al. identify different types of bad topics:
\begin{itemize}
  \item
    \textbf{Too general topics}: As a rule of thumb, words which occur more often do not add much meaning to a document.
    At the extreme, there are stopwords such as ``and'', ``the'', ``of'', etc., which do not provide value in terms of a good topic.
    Topics therefore should not contain too many general words, as in the following example: \\
    \hspace*{0cm} \framebox[1.1\width]{would one writes like think article know get time people} \\
    On the contrary, topics should also not contain words, which occur only once or twice in the entire corpus.
  \item
    \textbf{Mixed topics}: A mixed topic is a topic, which contains two topics. \\
    An example is the following topic \\
    \hspace*{0cm} \framebox[1.1\width]{cars engine speed oil water power air light}, \\
    which mixes a ``automobile'' topic with a ``energy'' topic.
  \item
    \textbf{Chained topics}: Chained topics are similar to mixed topics, in that they contain two topics.
    They also contain a word with several meanings, which occurs in both topics, thus creating an anchor point and an explanation, why the two topics occur together.
    In the following bad topic, the word ``windows'' constitutes such a word: \\
    \hspace*{0cm} \framebox[1.1\width]{file folder dos windows door house built}
  \item
    \textbf{Nonsensical topics}:
    Topics, which are not interpretable at all, because they contain uncommon words.
    This often happens, because of preprocessing errors leaving fragments in the text: \\
    \hspace*{0cm} \framebox[1.1\width]{max giz air uww fyn tct ahf fpl vmk pmf}
\end{itemize}

Topic coherence can be evaluated by humans.
Given the top words of a topic, human annotators can rate the topic on a three-point Likert scale 1-3 as either ``useless'', ``average'', or useful~\cite{Aletras2013}.
Averaging the annotations from several annotators yields the gold standard for the topic coherence of a set of words.
However, this an extensive and time-consuming approach in itself, and then needs to be done again for every new topic model with its own new topics.

The goal of a automatic topic coherence metric is to capture the usefulness of a topic for a human user automatically without depending on human annotators.
The base idea for such a automatic measure was introduced by Newman et al.~\cite{Newman2010}.
Just as human annotators, an automatic measure outputs a number indicating the coherence of given topic.
Usually this yields a value between zero and one.
The best automatic topic coherence measure is then that measure, which has the highest correlation with human ratings.
Correlation is measured using Pearson's correlation.
Basically, the human ratings are used as training data to decide, which automatic measure is the best.

To evaluate topic coherence automatically, all measures need an external knowledge base or corpus.
Newman et al.~\cite{Newman2010} tried metrics based on WordNet, Google searches and word-cooccurrences in Wikipedia articles, and found that using Wikipedia provides the best metric.

Röder et al.~\cite{Roder2015} built on this idea and developed an extensive survey to find the best method to combine the word-cooccurrences from Wikipedia into a coherence measure.
Using previously published topic ratings~\cite{Aletras2013,Chang2009,Newman2010,Rosner2013}, they could find the topic coherence measure with the highest correlation.
It is important to realize, that they could reuse previously published topic ratings, because the top words of a topic are independent of any topic model implementation or architecture.
The topic coherence is solely based on the coherence of a set of words.
Because of this, an automatic topic coherence measure can be selected based on an arbitrary set of topic ratings by human annotators and then be used for new topic models.

In total, Röder et al.~\cite{Roder2015} evaluated a total of 237912 coherence measures, which they created by parameterizing each step of the computation and then testing the cross product of all parameter settings.
The best metric, called $C_V$ in the original paper, had a correlation of $0.731$ with human ratings.
This shows, that topics with high values in the $C_V$ metric can usually be easily interpreted by humans.
We will use the $C_V$ metric in this master thesis to evaluate topic coherence.

Another possibility to evaluate topic models, is to use topic models for further, more end-user oriented tasks.
One example is document classification.
When using topic models for this, the document-topic proportions are used as features, i.e. the number of features is the number of topics.
Then, using a standard machine learning algorithm like support vector machines, a model can be trained and applied to new, unseen documents.
We will also use this method in our evaluation.


\section{Word embeddings}

In traditional language modeling, words are treated as atomic units by a discrete one-hot encoding.
Given a vocabulary $V$, each word $w$ is assigned a vector of length $|V|$ with all values set to zero except one to uniquely identify the word.
A typical architecture involves $n$-grams, which try to predict the next word based on previous words.
This approach has two disadvantages.
First, it limits the context to an arbitrary range.
When $n$ is too small, only the immediate preceding words are taken into account, failing to capture long-range word dependencies.
When $n$ is too large, the \emph{curse of dimensionality} becomes a problem: when representing a joint sequence of $n$ words, there are $|V|^n$ potential combinations.
As most $n$-grams are never observed, this leads to very sparse vectors.
Second, this type of language modeling does not capture similarity between words and also does not capture information about the relationship between words.
When using the one-hot encoding, both the words ``house'' and ``houses'' get a unique vector with no indication of similarity.
If one would employ cosine distance or euclidean distance to assess similarity in the vector space, all words would be equally similar.

In contrast to this, the idea of \emph{distributed representations} is to not represent objects by discrete counts but rather by continuous points in a vector space $\mathbb{R}^m$, where $m$ is much smaller than $|V|$.
The idea of distributed representations has a long history, dating back to the mid-eighties with work from Hinton and Rumelhart~\cite{Hinton1986,Rumelhart1988}.
Intuitively, instead of using one dimension for each atomic unit, distributed representations assign meanings to the dimensions and then recombine these meanings for a specific object.
%For example to represent , one dimension could indicate the size of an object, another one the color.
%With just three dimensions, one could represent small
%If one dimension indicates how green an object is and another dimension indicates whether an object is a flower, then one could represent a green flower with high values in both dimensions instead of requiring one dimension for red, blue, and green flowers.
Recently, the term \emph{word embeddings} was also used to describe the approach of embedding words in a vector space.
We will use the terms \emph{distributed representation}, \emph{word vectors} and \emph{word embeddings} interchangeably in this document.

The use of distributed representations for words was proposed by Bengio et al.~\cite{Bengio2003}.
In their work, they train a neural network to predict a word given the preceding context.
This prediction model is again based on the distributional hypothesis: words are similar, if they occur in the same contexts.
With this architecture, ``the system naturally learns to assign similar vectors to similar words''~\cite{Baroni2014}.
%https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis

In a series of papers, Mikolov et al.~\cite{Mikolov2013,Mikolov2013a,Mikolov2013b} provided a better neural architecture to train the word vectors.
Contrary to the trend of ever deeper models, a shallower model with no hidden layer was more successful in building better word vectors, because it can be trained on more data.
As Mikolov et al.~\cite{Mikolov2013a} state, ``it might not be able to represent data as precisely as neural networks, but can possibly be trained on much more data efficiently.''
Due to the state-of-the-art performance and the free open source implementation \emph{word2vec}\footnote{\url{https://code.google.com/archive/p/word2vec/}} this method gained popularity and will also be used in this thesis.

Mikolov et al.\ presented two models.
The continuous bag-of-words model (CBOW) works by predicting the center word in a symmetric context window.
The continuous skip-gram model works the other way round: given a word, it tries to predict the context.
We focus on the skip-gram model in the following, as it had the better evaluation results in the original paper.
In the skip-gram model, surrounding words have to be encoded in the word vector for each word or as Mikolov et al.~\cite{Mikolov2013} state ``vectors can be seen as representing the distribution of the context in which a word appears''.
% TODO: Skip-Grams with Negative Sampling

A nice property of the trained word vectors is, that linear relationships between words are kept in the vector space.
It was shown that
\begin{center}
       $vector("King") - vector("Man") + vector("Woman")$
\end{center}
is closest to the vector representation of the word ``queen''~\cite{Mikolov2013b}.
Interestingly, this relationship holds for both semantic and syntactic similarly~\cite{Mikolov2013a}, such as
\begin{center}
       $vector("houses") - vector("house") + vector("car")$
\end{center}
is closest to $vector("cars")$.
% Quote from webpage: This seems to be a great strength of neural networks: they learn better ways to represent data, automatically. Representing data well, in turn, seems to be essential to success at many machine learning problems. Word embeddings are just a particularly striking example of learning a representation.

Word embeddings have been used in many natural language and machine learning tasks.
They are a natural input to many machine learning algorithms, as they provide a better encoding than the traditional one-hot encoding.
Concrete use cases involve statistical language modeling, machine translation~\cite{Zou2013}, sentiment analysis~\cite{Maas2011} and paraphrase detection.

A word vector can be seen as a summary of the word with all its meanings.
Many applications require not only word vectors, but also summaries of longer chunks of text.
Therefore, it is necessary to embed sentences or documents in the word embedding space.
For this problem, Le and Mikolov~\cite{Le2014} proposed paragraph vectors.
In their work, paragraph can mean an arbitrary long text, ranging from sentences to entire documents.
Paragraph vectors are trained by adding an artificial word to each context, which represents the current paragraph.
These paragraph vectors can then be viewed as a summary of the meaning of the entire paragraph, which is useful for further downstream applications.
%for sentiment analysis by training logistic regression classifier on the vectors.
% http://sentic.net/sentire2011ott.pdf

\paragraph{Implementations}
The original implementation of the approach by Mikolov et al.~was open sourced under the name \emph{word2vec}.
It is implemented in C and can be used as a command line tool.
There also exists another implementation in the gensim package by {\v R}eh{\r u}{\v r}ek~\cite{Rehurek2010}.
These two implementations can run multi-threaded in the CPU.
There also exist some GPU implementations, however, these are not as stable.
Therefore, we will concentrate on the CPU implementations first.

\section{Comparison between LDA and word embeddings}

% At their foundations, LDA and word embeddings come from different backgrounds.
% LDA stems from Bayesian statistics, while word embeddings were first developed in the neural-network and deep learning community.
% Baroni et al.~\cite{Baroni2014} show, that both methods fundamentally are based on the assumption that ``semantically similar words tend to have similar contextual distributions''.
% However, the methods use this base hypothesis to come to different conclusions.
% LDA works on the bag-of-words assumption, i.e.\ it treats a document as a whole and does not use the order of the words.
% When making a prediction for the next word, LDA makes a global prediction based on the topic distribution in the current document.
% Word embeddings, on the other hand, generate the next word based on the context of the word and are per se local.
%%%% Edited out by Ralf, because of repetition. Probably try to incorporate the important parts from here (local-global, distributional hypothesis) in the earlier chapters.

Both LDA and word embeddings can provide distributed representations for words.
In LDA, this works by taking the topic distribution for a word as the embedding in the topic space.
However, this representation is not good at keeping linear relationships~\cite{Mikolov2013b,Mikolov2013a}.
Also, it tends to yield sparse vectors as LDA tries to keep the number of topics per word small.

Baroni et al.~\cite{Baroni2014} also introduce the classification of count-based methods and prediction-based methods in distributional semantic models.
Prediction-based methods try to set word vectors so that they are able to predict the context words.
Count-based methods are based on counting the occurrence of words and co-occurrence with other words.
Popular count-based methods typically use pointwise mutual information (PMI) and matrix transformation on the co-occurrence matrix.
While word embeddings are clearly a prediction-based method, LDA constitutes a hybrid type in this classification.
LDA is based on word counts and co-occurrences and treats words as discrete observations, however, the model parameters are chosen to maximize its predictive power.

Another aspect is the interpretability of the model.
LDA forces the elements in a vector to sum up to 1 and all values must be non-negative.
Thus, the embedding of a word in the topic space is easily interpretable by humans.
With a word vector of $[0~0~0.2~0.8]$ and given the meanings of a topic, a word can be interpreted as being used $20~\%$ in \emph{sports} and $80~\%$ in \emph{politics}.
When working with word embeddings, a vector like $[{-2.4}~0.3~1.3~{-0.1}]$ is not interpretable.
The arbitrary dimensions and values of a word embedding vector cannot be understood by humans.

Regarding the performance, LDA operates much slower than word embeddings.
LDA becomes very expensive on large data sets, because it needs to repeatedly iterate over the entire corpus.
The method by Mikolov has been successfully trained on a corpus with about 100 billion words.

% TODO: both somewhat use vector space model TODO: add seminal reference
% distributional semantic models (DSMs)

\paragraph{Typical operations}
Both models have different operations in which they perform well, which are standard operations for the respective models.

Word embeddings: find similar words
This is also explains the difference in evaluation.
Topic models are typically evaluated using topic coherence, i.e. how well the top words from the topics belong together and have a common, easily identifiable theme.
Word embeddings, on the contrary, are typically evaluated using word analogy reasoning tasks or word similarity tasks.
When building new topic models which use word embeddings, the goal is to incorporate a priori semantic information about words into the model.
The assumption is, that words which are similar to each other should have a higher probability to occur in the same topic together.


\section{Existing combinations}
The recent popularity of word embedding methods has led to a reevaluation of topic models.
There are two directions for research in this area.
% The existing related work, which combines topic modeling approaches with word embeddings, can be roughly divided in two directions.
One type of model tries to improve topic models by incorporating word vectors into the model.
As LDA is based on the bag-of-words model, no relationship between words is encoded in the model.
The prior knowledge about semantically similar words, that word embedding models provide, is used to create more coherent topic models.
The other type of model aims to improve word embeddings by incorparating ideas from topic models.
We summarize the prior work in this field in Table~\ref{table:tm_and_we_combinations}.\footnote{ToCoh: Topic Coherence; WordSim: Word Similarity; DocClass: Document Classification; DocClust: Document Clustering; Analogy: Word Analogy Reasoning; SentComp: Sentence Completion}
In the rest of section we will present some models in detail.
\begin{table*}
       \centering
       \caption{Overview over related work of combinations between topic models (TM) and word embeddings (WE) and how they have been evaluated}
       \begin{tabular}{|l||c|c||c|c|c|c|c|c|}
              \hline
              \textbf{Paper}                  & \textbf{TM} & \textbf{WE} & \textbf{ToCoh} & \textbf{WordSim} & \textbf{DocClass} & \textbf{DocClust} & \textbf{Analogy} & \textbf{SentComp} \\
              \hline
              Blei et al.~\cite{Blei2003}     & \cmark &  &                &                  & \specialcell{\cmark\\\textsc{Reuters}}  &                   &  & \\
              \hline
              Mikolov et al.~\cite{Mikolov2013a} &     & \cmark &                &           &                      &                   & \cmark & \specialcell{\cmark\\\textsc{Microsoft}\\\textsc{sentence}\\\textsc{completion}} \\
              \hline \hline
              Das et al.~\cite{Das2015}       & \cmark &  & \specialcell{\cmark\\Wikipedia\\co-occurrence} &      &                   &                   & & \\
              \hline
              Nguyen et al.~\cite{Nguyen2015} & \cmark &  & \specialcell{\cmark\\Wikipedia\\co-occurrence} &      & \specialcell{\cmark\\\textsc{20-News-}\\\textsc{Group},\\\textsc{TagMyNews}} & \specialcell[h]{\cmark\\Same as\\left} & & \\
              \hline
              Sridhar~\cite{Sridhar2015}      & \cmark &  & \specialcell{\cmark\\Manual\\annotation}               &                  &                   &  & & \\
              \hline
              Moody~\cite{Moody2016}          & \cmark & \cmark & \specialcell{\cmark\\Palmetto\footnotemark\\evaluation tool}         & \specialcell{\cmark\\Only\\qualitative} &                      &         & & \\
              \hline
              Liu et al.~\cite{Liu2015} &     & \cmark &                & \specialcell{\cmark\\\textsc{SCWS}}    & \specialcell{\cmark\\\textsc{20-News-}\\\textsc{Group}}               &                   &  & \\
              \hline
              Cheng et al.~\cite{Cheng2015} &     & \cmark &                & \specialcell{\cmark\\\textsc{SCWS} +\\paraphrase\\detection\\\textsc{MSRPC}}    &  &                   &  & \\
              % • Nguyen et al.~\cite{Nguyen2015}} & • \parbox[t]{7cm}{Conceptual Text Understand in Distributional Semantic Space, Cheng et al.~\cite{Cheng2015}} \\
              % • \parbox[t]{7cm}{Unsupervised Topic Modelling for Short Texts Using Distributed Representations of Words, Sridhar~\cite{Sridhar2015}} & • lda2vec, Moody~\cite{Moody2016} \\
% • Topical Word Embeddings, Liu et al.~\cite{Liu2015}
              \hline
       \end{tabular}
       \label{table:tm_and_we_combinations}
\end{table*}
\footnotetext{The online evaluation tool can be accessed at \url{http://palmetto.aksw.org/palmetto-webapp}}

\subsection{Improved topic models}

We will focus on models, which yield improved topic models and present them in the following paragraphs.
\paragraph{Gaussian LDA}
Das et al.~\cite{Das2015} proposed a method named Gaussian LDA.
In this approach, a topic is no longer a distribution over words in the vocabulary, but a Gaussian distribution in the word embedding space.
Also, words are no longer atomic units, but are represented by their corresponding pre-trained word embeddings.
The authors use word embeddings trained on the English Wikipedia, which are not published.
Each topic is associated with a mean $\mu_k$ and a covariance matrix $\Sigma_k$.
The prior distribution over the means is a normal distribution, and the prior distribution over the covariances is an inverse Wishart distribution.

Three hyperparameters have to be set for Gaussian LDA.
$\nu$ represents the degrees of freedom for the inverse Wishart distribution and is typically set to the number of dimensions of the word embeddings.
$\Psi$ represents the scale matrix of the inverse Wishart distribution and is set to the identity matrix.
$\kappa$ represents the smoothing parameter for the counts of words assigned to a certain topic.
With these hyperparameters, the generative process of Gaussian LDA can be summarized as follows:
\begin{enumerate}
       \item Fix number of topics $K$ in the document collection
       \item Choose covariance matrix $\Sigma_k \sim \mathcal{W}^{-1}(\Psi, \nu)$ for each topic $j \in \{1~..~K\}$
       \item Choose mean $\mu_k \sim Normal(\mu, \frac{1}{\kappa} \Sigma_k)$ for each topic $j \in \{1~..~K\}$
       \item Now, for each document $d$:
       \begin{enumerate}
              \item Choose topic distribution $\theta_d \sim Dirichlet(\alpha)$
              \item For each position $i$ in the document
              \begin{enumerate}
                     \item Choose topic $z_{d,i} \sim Multinomial(\theta_d)$
                     \item Choose word $w \sim Normal(\mu_{z_{d,i}}, \Sigma_{z_{d,i}})$
              \end{enumerate}
       \end{enumerate}
\end{enumerate}

These parameters are inferred using Gibbs sampling as in standard LDA with the following sampling equation: % together with linear algebra tricks
\begin{equation*}
  p(z_{d,i} | z_{-(d,i), V_d}) \propto (n_{k,d} + \alpha_k) \times t_{\nu_k - M + 1}\Big(v_{d,i} \Big| \mu_k, \frac{\kappa_k + 1}{\kappa_k} \Sigma_k\Big)
\end{equation*}
where $z_{-(d,i)}$ are all topic assignments in all documents except the current word, $V_d$ is the sequence, $M$ is the number of dimension of the word vectors.
The evaluation of the multivariate $t$ distribution requires the determinant and the inverse of the covariance matrices $\Sigma_k$, which is an $\mathcal{O}(M^3)$ operation.
Also, this parameters need to be updated after each word.
Even though the authors provide some performance improvements using linear algebra identities, inference in Gaussian LDA is very slow.
We will compare the runtime with other models in TODO.

\paragraph{Latent Feature Topic Models}
Another approach is taken by Nguyen et al.~\cite{Nguyen2015}.
They use word embeddings to sample words not only from the multinomial topic distribution, but also from the embedding space.
Instead of directly sampling a word from the topic-word distribution of the chosen topic, they introduce a Bernoulli parameter $s \sim Ber(\lambda)$.
This parameter decides, whether the word is sampled as usual from the topic-word distribution or from the latent feature vectors.
When sampling in the embedding space, the authors need to define a distribution over all the words.
This is achieved by introducing one topic vector $\tau_t$ for all topics.
This topic vector lies in the same space as the word embeddings.
Then, to sample in the embedding space, the model samples from the following softmax-normalized multinomial word distribution:
\begin{equation*}
    Multinomial(w | \tau_t, \omega) = \frac{exp(\tau_t * \omega_w)}{\sum_{w' \in W} exp(\tau_t * \omega_{w'})},
\end{equation*}
where $\tau_t$ is the representation of topic $t$ in the word embedding space and $\omega$ are the pretrained and fixed word embedding vectors.
The topic embeddings $\tau_t$ are updated after each iteration of Gibbs sampling via a maximum likelihood estimation based on all other parameters given.
Here, the topic assignments are fixed.
The negative log likelihood including $L_2$ regularization for each topic vector $\tau_t$ is given as:
\begin{equation*}
    L_t = - \sum_{w \in W} K^{t,w} \Big(\tau_t * \omega_w - \log \sum_{w' \in W} \exp (\tau_t * \omega_{w'}) \Big) + \mu * || \tau_t ||^2,
\end{equation*}
and is minimized with respect to the topic vectors.
Optimization is done via L-BFGS~\cite{Liu1989}.

Using the the topic vectors, the generative process can be summarized as follows:
\begin{enumerate}
    \item Fix number of topics $K$ in the document collection
    \item Choose word distributions $\phi_j \sim Dirichlet(\beta)$ for each topic $j \in \{1~..~K\}$
    \item Now, for each document $d$:
    \begin{enumerate}
        \item Choose topic distribution $\theta_d \sim Dirichlet(\alpha)$
        \item For each position $i$ in the document
        \begin{enumerate}
            \item Choose topic $z_{d,i} \sim Multinomial(\theta_d)$
            \item Choose binary variable $s_{d_i}$, which decides whether the word is sampled from the multinomial as usual or from the word embedding space
            \item Choose word $w \sim (1 - s_{d_i}) * Multinomial(\phi_{z_{d,i}}) + s_{d_i} * Multinomial(\tau_{z_{d_{i}}} \omega^T)$
        \end{enumerate}
    \end{enumerate}
\end{enumerate}
The authors use pretrained vectors from \emph{word2vec}~\cite{Mikolov2013a} and \emph{GloVe}~\cite{Pennington2014} and conclude that none of these works better than the other.

\paragraph{Nonparametric Spherical Model}
Batmanghelich et al.~\cite{Batmanghelich2016} introduce a  introduce nonparametric spherical topic modelling.
They build on top of Gaussian LDA, but argue that Gaussian distributions do not capture the embedding space very well.
Instead they propose the von Mises-Fisher distribution to capture topics in the embedding space.
The von Mises-Fisher distribution is a distribution on the hypersphere with a mean direction $\mu$ and a concentration parameter $\kappa$.
If there are only two dimensions, the distribution reduces to the von Mises distribution on the circle.
Using the von-Mises Fisher distribution requires all word vectors to be normalized so they lie on the $M$-dimensional unit hypersphere.

Intuitively, the von-Mises Fisher distribution is a good distribution choice, because it is built on the .
The Gaussian LDA model tries to group words in a topic, which are near each other in the embedding space, where ``near'' is measured by the euclidean distance.
However, word similarity in embedding models is usually not measured by euclidean distance, but rather by cosine distance~\cite{Mikolov2013a}, which is exactly what is captured by the von Mises-Fisher distribution.

In the spherical topic model, each topic is represented by a von Mises-Fisher distribution with parameters $\mu_k$ and $\kappa_k$.
A log-normal prior is placed on $\kappa_k$ and a von Mises-Fisher prior is placed on the $\mu_k$'s.
In contrast to the previous models, this model is not based on LDA but rather on hierarchical Dirichlet processes.
These models allow the number of topics to be determined automatically.
% To allow a variable number of topics the document-topic distribution is sampled.
All together, the generative process looks as follows:
\begin{enumerate}
    \item Choose base distribution $\beta \sim GEM(\gamma)$ from stick-breaking distribution
    \item Choose topic direction on unit sphere $\mu_k \sim vMF(\mu_0, \kappa_0)$ for all topics $k$
    \item Choose topic concentration $\kappa_k \sim log\mbox{-}normal(m, \sigma)$ for all topics $k$
    \item Now, for each document $d$:
    \begin{enumerate}
        \item Choose topic distribution $\theta_d \sim DP(\alpha, \beta)$
        \item For each position $i$ in the document
        \begin{enumerate}
            \item Choose topic $z_{d,i} \sim Multinomial(\theta_d)$
            \item Choose word $w \sim vMF(\mu_{z_{d,i}}, \kappa_{z_{d,i}})$
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

The authors use 50-dimensional word embeddings, which they trained on Wikipedia.
The word vectors are not available.

\paragraph{Generative Topic Embedding}
Li et al.\cite{Li2016} propose generative topic embeddings.
They call their system TopicVec.
The TopicVec model is less focused on topic coherence, but rather on document representations.
They treat a document as a ``bag-of-vectors'' and learn a topic model and the word embeddings in parallel.
This is in contrast to the other models, which are always built on pre-trained word vectors.
The authors claim, that this method ``exploits the word collocation patterns both at the level of the local context and the global document''~\cite{Li2016}.

The vectors are learned by maximizing the probability of a word occurring with its context, according to the following formula:
\begin{equation*}
  P(w_c | w_0 : w_{c - 1}, z_c, d_i) = P(w_c) * \exp \bigg( v_{w_{c}}^T * \Big( \sum\limits_{l=0}^{c-1} v_{w_l} + t_{z_c} \Big) + \sum\limits_{l=0}^{c - 1} a_{w_{l} w_c}  + r_{z_c} \bigg),
\end{equation*}
where $c$ is the context window size, $w_c$ is the word at the last position of the current context window, $v_{w_c}$ is the word's vector embedding, $t_{z_c}$ is the topic embedding vector of the word at position $c$ and $r_{z_c}$ is a normalizing constant.
The terms $a_{w_l w_c}$ are bigram residuals, which capture the relationship between each context word and the target word explictly.
Similarly to other methods TopicVec also encodes a topic with a vector $t_k$ in the embedding space.
Topic vectors


The generative process looks as follows:
\begin{enumerate}
    \item Fix number of topics $K$ in the document collection
    \item Choose topic vector $t_j \sim Unif(S_{\gamma})$ from hypersphere of radius $\gamma$ for each topic $j \in \{1~..~K\}$
    \item Now, for each document $d$:
    \begin{enumerate}
        \item Choose topic distribution $\theta_d \sim Dirichlet(\alpha)$
        \item For each position $i$ in the document
        \begin{enumerate}
            \item Choose topic $z_{d,i} \sim Multinomial(\theta_d)$
            \item Choose word $w \sim Multinomial()$
        \end{enumerate}
    \end{enumerate}
\end{enumerate}


\paragraph{Gaussian Mixture Model in Embedding Space}
Another idea is proposed by Sridhar~\cite{Sridhar2015}.
They start by training word embeddings.
Afterwards they fit a Gaussian mixture model directly on the word embedding space.
A topic is represented by mean and covariance matrix.
In contrast to Das et al.~\cite{Das2015}, Sridhar~\cite{Sridhar2015} assumes the topics to be fixed given the word embeddings, while Das et al.~\cite{Das2015} only take the embeddings as a basis and still learn the topics based on the word co-occurrences in the corpus.


\subsection{Improved word embeddings}
We now focus on methods for better word embeddings.
Liu et al.~\cite{Liu2015} propose topical word embeddings.
These embeddings make use of word topics inferred via LDA to generate better word vectors.
They propose three different architectures.
The first method regards topic as pseudo words, which are added to the context of each word.
Thus, an embedding for a word and its topic is learned independently.
The second method considers each pair of word and topic as a pseudo word.
The third method works the same way, except that it learns only one word embedding vector per word and one topic embedding vector per topic.
These two vectors are then combined for the vector of the word-topic pair.
To the authors' surprise, the simplest method one performs best in their word similarity and text classification benchmarks.
Using the additional topic information when training word embeddings has practical benefits for similarity evaluation.
When the model is asked for similar words to the word ``apple'', it returns ``peach'', ``juice'', ``strawberry'' or it returns ``mac'', ``ipod'', ``android'' depending on the topic of the word.

A similar idea is used by Cheng et al.~\cite{Cheng2015}.
In fact, the first model they propose is the same as the first model from Liu et al.~\cite{Liu2015}.
In addition to some other new models, which are similar to Cheng et al.'s work, they also propose new models, which do not assume independence between word and concept embeddings.
These \emph{generative word-concept skip-gram} models perform best in their word-similarity and paraphrase detection benchmarks.

Moody~\cite{Moody2016} proposes lda2vec.
His method learns word vectors together with learning topic vectors and an intuitive topic-based document interpretation.
Even though trained in the word embedding space, the topic-document vectors are still kept parse to be interpretable for humans.
However, the method is not evaluated extensively and it is unclear, whether this method yields improved word vectors.

% FROM MIKOLOV COLING 2014 tutorial
% Sentence-level representations
% • To obtain sentence level representations, we can add unique tokens
% to the data, one for each sentence (or short document)
% • These tokens are trained in a similar way like other words in the skipgram
% or CBOW models, just using unlimited context window (within
% the sentence boundaries)


%one is supervised, the other one is unsupervised

%``A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semanti- cally similar words tend to have similar contex- tual distributions (Miller and Charles, 1991).''
%``It remains to be seen whether the two types of models are complementary in the errors they make, in which case combined models could be an interesting av- enue for further work.''
%there has been a shift from the LDA to the word embeddings


% \begin{itemize}
       % \item
       %        Directly run a GMM on the word embedding space --> See Gaussian LDA
       % \item
       %        Run LDA first.
       %        Then run word embedding training, except that the word also has to predict the surrounding context.
       %        Problem: LDA performance
       % \item
       %        Run LDA first.
       %        Then run word embedding training, except that the same word from different topics gets a unique token
       %        Problem: LDA performance
       % \item
       %        Try to interpret word embedding dimensions using topic models.
       %        This requires a labelled topic model, i.e.\ topic 40 is about sports.
       %        Then try to put a label on a word embedding dimension, for example, the large the value in dimension x the more about sports.
       % \item
       %        Run LDA, then calculate average word vector and covariance matrix for all topics.
       %        Check, how well the topics are distributed in the topic space.
       %        This also allows to assign unseen words to a topic.
% \end{itemize}

% \section{Preprocessing}
% No-stemming (does harm in both \emph{word2vec} and topic modelling), as it destroys the difference between Apple and apples and (TODO: other example) as well as small/smallest.
% Quoting lots of stuff from the lda2vec

% \section{Enhancing topic models?}
% Then we need evaluation for topic models and show that they are better
% Use pretrained word vectors to improve~\cite{Das2015}



%\include{chapters/introduction}
%\include{chapters/related_work}
%\include{chapters/definitions}
%\include{chapters/dataset_creation}
%\include{chapters/search_methods}
%\include{chapters/recommending}
%\include{chapters/future_work}

\section{Bayesian optimization}
\begin{itemize}
  \item Good for noisy black-box functions, when evaluations are costly, no derivative and the problem is non convex
  \item Builds a statistical model for the function from the hyperparameter space to the evaluation function
  \item By using the information of all previous runs, the framework can suggest a new point to which provides max information
  \item balances \emph{exploration} (checking previously unknown points in the hyperparameter space) vs. \emph{exploitation} (using the current best values to check nearby points in the hyperparameter space)
  \item $\Rightarrow$ fewer experiments are necessary
  \item \url{https://github.com/HIPS/Spearmint}
\end{itemize}


\chapter{Word embeddings in topic models}

In this chapter we will present our analysis on the spread.
Afterwards we will present our new topic model approach, which is based on re-sampling words using information from the word embedding models.
We call our new model WELDA (word embedding LDA).
Afterwards we will analyze the different topic models and word embedding-enhanced topic models in two standard tasks for topic models: topic coherence and document classification.

\section{WELDA}
\subsection{Motivation}
As word embeddings and LDA topic modeling are different techniques with no inherent relationship to each other, we start with exploring how the results of one can be interpreted in the other model.
\begin{table*}
       \centering
       \caption{Word similarity in topic models and word embeddings}
       \begin{tabular}{llll}
       \hline
       \textbf{}                                                                                  & \textbf{} & \multicolumn{2}{l}{\textbf{Word embedding similarity}} \\
                                                                                                  &           & Low                       & High                       \\ \hline
       \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Topic model\\ similarity\end{tabular}}} & Low       &                           &                            \\
                                                                                                  & High      &                           &
       \end{tabular}
       \label{table:tm_and_we_word_similarities}
\end{table*}

Without completely disregarding the bag-of-words document information




\subsection{Algorithm}
We base our new topic modeling technique on the observations from the last chapter.
Word embedding models are trained on predicting a certain,
That means, if we look for similar words for a given word, the model predicts a word which could have been at the same position.

Now, to transfer probability mass more towards topic words, we resample 
Instead of replacing every single word based on a similar word, we first try to get the general theme of the topic.
We do this by creating 

WELDA needs a generative model of the topic space ... because we resample


bad topic words
For example, imagine
Two highly similar brands
however, never occuring together
but in very similar contexts

Often it is only a few words, which make out a topic.
For example, the topic words TODO make out only xxx percent of the words in the documents, where this topic is the dominating one.

For each topic, we learn a multivariate probability distribution in the embedding space.
The distribution parameters are estimated
The choice of distribution can be varied and will be explained later.

We now describe the algorithm in more detail:
\begin{algorithm}
  \caption{Initialization for the WELDA model, to be run before the Gibbs sampling}
  \label{alg:welda_init}
  \begin{algorithmic}[1]
    \Procedure{Init}{array of words $words$, word embeddings $v$}
    \For{i $\in$ \{ 1 .. $|words|$ \}}
      \State{$embedding\_matrix[i] = v.get\_word(words[i])$}
    \EndFor
    \State{$embedding\_matrix \gets PCA(embedding\_matrix, n)$}
    \State{$lsh \gets \textit{initialize LSH structure for nearest neighbour search}$}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Gibbs sampling algorithm for the WELDA model}
  \label{alg:welda_gibbs_sampling}
  \begin{algorithmic}[1]
    \Procedure{Sampling}{}
    \State $\textit{initialize topic variables } z_{d_i} \textit{ by running standard LDA for } m \textit{ iterations}$
    \For{$n \in \{\textit{ 1 .. N }\}$}
      \State{\textsc{EstimateDistributionParameters()}}
      \For{$d \in \{\textit{ 1 .. $|D|$ }\}$}
        \For{$w\_idx \in \{\textit{ 1 .. $N_d$ }\}$}
          \State{$word\_id \gets D[d][w\_idx]$}
          \State{\textit{decrement doc-topic and topic-word count for old topic}}
          \State{\textit{// sample new topic based on adapted counts}}
          \State{$topic\_id \gets sample\_new\_topic()$}
          \If{$coin\_flip(\lambda)\textit{ is successful}$}
            \State{$sample \gets sample\_from\_distribution(topid\_id)$}
            \State{$\textit{// update $word\_id$, as if we had seen another word}$}
            \State{$word\_id = lsh.find\_nearest\_word(sample)$}
          \EndIf
          \State{\textit{increment doc-topic and topic-word count for new topic}}
        \EndFor
      \EndFor
    \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Estimating distribution parameters for each topic, run before each Gibbs sampling iteration}
  \label{alg:welda_parameter_estimation}
  \begin{algorithmic}[1]
    \Procedure{EstimateDistributionParameters}{}
    \For{$topic\_id \in \{\textit{ 1 .. $K$ }\}$}
      \State{$top\_words \gets$ \textit{Get top $TODO$ words in topic $topic\_id$}}
      \State{\textit{Get}}
    \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

There are several configurable parameters in the algorithm, which we will detail in the following:
\begin{itemize}
  \item \textbf{Choice of topic distribution}:
    We use two distributions for the topic distributions.
    First, the well-known multivariate Gaussian distribution.
    Second, by inspiration of Batmanghelich et al. \cite{Batmanghelich2016}, we use the multivariate von Mises-Fisher distribution.
    We demonstrate these two in detail in the next sections.
  \item \textbf{Resampling probability $\lambda$}:
    The parameter $\lambda$ governs, how often a word is resampled from the topic distributions.
    With $\lambda = 1.0$, no single word is taken from the text, rather all words are sampled from the embedding space.
    Running WELDA with $\lambda = 0.0$ corresponds to standard, unmodified LDA.
    This parameter decides, how much extra information from the word embeddings is leaked into the topic model.
  \item \textbf{Number of top words for distribution estimation $N_{top}$}:
    We are using the top words of each topic to estimate the distributions.
    Usually only the first
    TODO: Number of top words vs. topic coherence.
  \item \textbf{Number of dimensions for dimensionality reduction $N_{pca}$}:
  \item \textbf{Dimensionality reduction method}:
  \item \textbf{Distance function for the nearest neighbour search}:
    Euclidean distance or cosine similarity.
  \item \textbf{Number of iterations to run LDA}:
    We usually run $M = 1500$ iterations, as this yields stable topic assignments, and is also used in literature~\cite{Nguyen2015}.
    As visible in Figure~\ref{fig:ll_lda_convergence}, the log-likelihood has already converged with that many iterations.
    \begin{figure}
           \centering
           \includegraphics[width=0.5\textwidth]{figures/ll_lda_convergence.png}
           \caption{Convergence of the LDA model on the 20-news corpus}
           \label{fig:ll_lda_convergence}
    \end{figure}
\end{itemize}


\subsection{WELDA with Gaussian distributions}
\subsection{WELDA with von Mises-Fisher distributions}
\subsection{WELDA with Gaussian mixture models}

% \subsection{Different strategies}
% Write about different strategies
% Only use best word:
% model.google.model.welda.lambda.only-most-similar-we
% Use all words, no filtering:
% model.google.model.welda.lambda.full-no-filtering

% Gaussian WELDA

% Von-Mises-Fisher WELDA

\section{Implementation details}
\subsection{Nearest neighbour search}
https://www.youtube.com/watch?v=Arni-zkqMBA

Figure~\ref{fig:lsh_vs_kdtree} shows the ratio of the runtime of the k-d tree and LSH.
Note the logarithmic y-axis, so the ratio actually increases very fast.
We see, that up to five dimensions including, the k-d tree is faster for looking up the nearest neighbour.
For six or more dimensions, LSH is preferred.
\begin{figure}
       \centering
       \includegraphics[width=0.6\textwidth]{figures/lsh_vs_kdtree.png}
       \caption{Convergence of the LDA model on the 20-news corpus}
       \label{fig:lsh_vs_kdtree}
\end{figure}
\subsection{Vocabulary mismatch between word embeddings and topic model}

Matching vocabulary is important part of preprocessing.
TODO: Show citations

\begin{algorithm}
  \caption{Looking up words in the embedding model}
  \label{euclid}
  \begin{algorithmic}[1]
    \Procedure{Word2Vec.get\_word}{word $w$}
    \If{w exists in embeddings}
      \State{}
      \Return {array of floats for word w}
    \ElsIf{w.capitalized() exists in embeddings}
      \State{}
      \Return {array of floats for word w.capitalized()}
    \ElsIf{w.uppercased() exists in embeddings}
      \State{}
      \Return {array of floats for word w.uppercased()}
    \EndIf
    \State{\textbf{Exception}: Error in preprocessing}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Background topic for corpus-specific stop-words}
\subsection{Running LDA as a prerequisite}
Explain why we do not run WELDA from the start - run experiment!

\subsection{Implementation}

gensim
Mallet
Deep Learning 4j

\chapter{WELDA Discussion}


\section{Topic Coherence}
Topic coherence is a standard task to
\subsection{Experiment setup}
\subsubsection{Data Sets}
\subsubsection{Topic coherence evaluation}

We will use topic coherence as a metric to evaluate the quality of our topics.
As a topic coherence metric, we use the $C_V$ metric from Röder et al.~\cite{Roder2015}, which did an extensive survey on topic coherence metrics.
Their $C_V$ metric had the highest correlation with human annotators ($r_{Pearson} = 0.731$).

At first, the number $N$ of top words to be considered for the measure must be chosen.
We choose $N=10$, because ``the evaluation of topic quality is harder, if the number of top words $N$ is small''~\cite{Roder2015} and because it is a common choice in the literature~\cite{Roder2015,Aletras2013}.
We denote the set of top words with $W$.
The topic coherence evaluation for one set of ten top words is calculated in four steps:
\begin{enumerate}
  \item \textbf{Word Segmentation}:
    The set $W$ of top words is segmented into pairs of word-sets, for which the coherence is calculated separately in the following step.
    The words are separated as follows:
    \begin{equation*}
      S = \{ (W', W^*) | W' = \{w_i\}, w_i \in W; W^* = W \}
    \end{equation*}
    Assuming a set $W = \{ car, automobile, fuel \}$ of $N = 3$, one tuple in $S$ would be $\{ (\{car\}, \{car, automobile, fuel\})\}$, which represents the question \emph{How well do the words ``car'', ``automobile'', and ``fuel'' support the word ``car''?}
  \item \textbf{Probability Estimation}:
    To estimate the probability of a word, the $C_V$ metric is based on word occurrences and co-occurrences in the English Wikipedia.
    The probability of a word is estimated using document frequency, i.e. the number of documents a word occurs in divided by the total number of documents.
    Similarly, the probability of a set of words is estimated as the number of documents all words in the set occur divided by the total number of documents.
    The $C_V$ metric does not use the original Wikipedia articles as documents, but rather runs a sliding window over the Wikipedia articles.
    The sliding window advances one token at a time and creates a new virtual document.
    The size of the sliding window is set to $110$ tokens.
    Using these new virtual documents has two advantages.
    First, it creates many more documents, as the average Wikipedia article is longer than $110$ tokens.
    Second, and more importantly, it exploits context information.
    Only words, which occur near each other are taken into account.
    Thus, words which occur far apart in a long document, do not increase their joint probability.
    This approach is called \emph{Boolean sliding window}.
  \item \textbf{Confirmation Measure}:
    Now, the coherence of a tuple $(W', W^*)$ needs to be calculated.
    $C_V$ uses the normalized pointwise mutual information, often referred to as NPMI, which is defined as follows:
    \begin{equation*}
      PMI(W_i, W_j) = \log \frac{P(W_i, W_j) + \epsilon}{P(W_i) * P(W_j)}
    \end{equation*}
    \begin{equation*}
      NPMI(W_i, W_j) = \frac{PMI(W_i, W_j)}{- \log(P(W_i, W_j) + \epsilon)}
    \end{equation*}
    The $\epsilon$ is added to avoid a zero inside the logarithm, and is set to $\epsilon = 10^{-12}$.
    The NPMI is used indirectly in the calculation of $C_V$.
    Instead of evaluating it directly on a given tuple $(W', W^*)$, it used to define a context vector for each top word.
    With the top word set $W$, the context coherence vector $\vec{v}$ of a segment set $\tilde{W}$ is defined as follows:
    \begin{equation*}
      \vec{v}_j(\tilde{W}) = \sum\limits_{\tilde{w} \in \tilde{W}} NPMI(\tilde{w}, w_j); j \in \{ 1, \ldots, | W | \}
    \end{equation*}
    For example, the vector for the word set $\{ car, fuel \}$ would be
    \begin{equation*}
      \vec{v}_j(\{ car, fuel \}) =
      \begin{pmatrix}
        NPMI(car, car) + NPMI(fuel, car) \\
        NPMI(car, automobile) + NPMI(fuel, automobile) \\
        NPMI(car, fuel) + NPMI(fuel, fuel) \\
      \end{pmatrix}
    \end{equation*}
    The dimension of the vectors $\vec{v}$ is always equal to the number $N$ of top words.
    Using these context coherence vectors, the coherence of a pair $(W', W^*)$ is computed using cosine similarity:
    \begin{equation*}
      \vec{u} = \vec{v}(W')
    \end{equation*}
    \begin{equation*}
      \vec{w} = \vec{v}(W^*)
    \end{equation*}
    \begin{equation*}
      coherence(\vec{u}, \vec{w}) = \frac{\vec{u} \cdot \vec{w}}{||\vec{u}|| * ||\vec{w}||}
    \end{equation*}
    Compared to directly using the NPMI, using an indirect measure has the advantage, that it assigns a high score also to word pairs, which occur together only rarely, but still occur in the same contexts.
    This can be the case for products or brands in the same category~\cite{Roeder2015}.
    While they seldom occur together, their contexts are similar, thus a high coherence score is beneficial.
  \item \textbf{Averaging}:
    As we have $N$ pairs of tuples $(W', W^*)$, the $coherence$ values need to be averaged.
    $C_V$ uses arithmetic mean.
\end{enumerate}

As $C_V$ outputs one coherence value for each topic, we need to average again to obtain a metric for the entire topic model.
Again, we use arithmetic mean here.

We directly use the author's implementation of $C_V$ in their Palmetto\footnote{\url{https://github.com/AKSW/Palmetto}} topic quality measuring toolkit.
Table~\ref{table:best_worst_cv_topics} shows the best and worst topics measured by $C_V$ in the 20news corpus.
\begin{table}[]
  \centering
  \caption{Best and worst topics in our best topic model on the 20news-corpus with 50 topics according to $C_V$}
  \begin{tabular}{ll}
  \hline
  \textbf{Coherence} & \textbf{Top words} \\ \hline
  0.79               & god, faith, moral, morality, truth, christians, religion, atheists, belief, bible \\
  0.76               & team, hockey, players, season, game, nhl, league, teams, games, play \\
  ...                &                    \\
  0.33               & good, find, figuring, exciting, sure, things, anybody, mistake, hopefully, decent \\
  0.30               & radar, detector, realizing, saturn, make, insurance, car, detectors, dealer, weight \\ \hline
  \end{tabular}
  \label{table:best_worst_cv_topics}
\end{table}

\subsubsection{Preprocessing}

For the topic models, the entire corpus was lowercased and tokenized.
A token was each sequence of letter or punctuation characters not divided by a whitespace character.
Trailing punctuation at the end of a token was removed.
We then removed stop words, using the stop words list from the Python nltk~\cite{Bird2009} toolkit.
The list contains 150 general English stop words.
With this preprocessing each document was represented as a bag-of-words, i.e. multiple occurrences of the same word were taken into account.

TODO: Word embeddings?


\subsubsection{Standard LDA as baseline}
As a baseline, we started with evaluating the topic coherence of the standard LDA algorithm.
We also used this to determine good values for the LDA parameters $\alpha$ and $\beta$.
We ran a grid search using $[0.002, 0.005, 0.01, 0.02, 0.05]$ as possible values for $\alpha$ and $[0.002, 0.005, 0.01, 0.02, 0.05, 0.1]$ as possible values for $\beta$.
Interestingly, there is no clear trend visible.
The best topic coherence results were achieved with $(\alpha=0.01,\beta=0.02)$, $(\alpha=0.005,\beta=0.005)$, and $(\alpha=0.02,\beta=0.1)$.
However, all results were in a very close range, with a standard deviation of all results of $0.006$.
This impression is further strengthened by Figure~\ref{fig:standard_lda_alpha_beta}, where we plot the average TC coherence for a fixed $\alpha$ or $\beta$.
Again, no trend is recognizable.
Because of this, we base our following experiments on the values $(\alpha=0.02, \beta=0.02)$, as setting the parameters to $1/K$ is a common setting in the literature.
%TODO: Cite
The baseline topic coherence score for this is $TC = 0.419$.

The results are similar for the \textsc{Nips} corpus.
We again choose $(\alpha=0.02, \beta=0.02)$ and start with a topic coherence baseline of $TC = 0.406$.

\begin{figure}
       \centering
       \includegraphics[width=\textwidth]{figures/standard_lda_alpha_beta.png}
       \caption{Average topic coherence for a fixed $\alpha$ and $\beta$ after the grid search.}
       \label{fig:standard_lda_alpha_beta}
\end{figure}

\subsection{Gaussian WELDA}

We will start with the analysis of our basic algorithm, which uses a Gaussian distribution to estimate the topic-word distribution in the embedding space.
Only then we will evaluate the more complex von Mises-Fisher distribution and the Gaussian mixtures.
Unless noted otherwise, all our experiments are run with $1500$ standard LDA iterations and then $200$ following WELDA iterations.

\subsubsection{Influence of the resampling parameter $\lambda$}
We started the evaluation with the influence of the $\lambda$ parameter.
It must be noted, that the $\lambda$ parameter the algorithm is configured with, is not the final $\lambda$ parameter.
This is because we always resample corpus-specific stop words, that we identified from the background topic.
The plots in the following always use the actual $\lambda_{act}$, i.e. how often was a word replaced during sampling, either because it was a stop word or because the coin toss was successful.
The $\lambda_{act}$ values are slightly higher than the original $\lambda$ values, with diminishing differences for higher $\lambda$. See Table~\ref{table:actual_lambda} for the differences.
\begin{table}[]
  \centering
  \caption{Comparison of $\lambda$ and $\lambda_{act}$}
  \label{table:actual_lambda}
  \begin{tabular}{l|llllllllllll}
    $\boldsymbol{\lambda}$     & 0.00 & 0.001 & 0.1  & 0.2  & 0.3  & 0.4  & 0.5  & 0.6  & 0.7  & 0.8  & 0.9  & 1.0 \\
    $\boldsymbol{\lambda_{act}}$ & 0.00 & 0.11  & 0.20 & 0.29 & 0.38 & 0.46 & 0.55 & 0.64 & 0.73 & 0.82 & 0.91 & 1.0
  \end{tabular}
\end{table}

\begin{figure}
       \centering
       \includegraphics[width=\textwidth]{figures/welda_gaussian_lambda_20news.png}
       \caption{Topic coherence development for Gaussian WELDA and \textsc{20news} as embedding corpus. Topic coherence was evaluated every 20 iterations.}
       \label{fig:welda_gaussian_lambda_20news}
\end{figure}
% TODO: Mention plots are scaled by 100.

We start with the analysis of with the topic models using the embeddings trained on the \textsc{20news} corpus.
The results are plotted in Figure~\ref{fig:welda_gaussian_lambda_20news}.
There are several interesting observations in the plot.
First, with $\lambda = 0.0$ the topic coherence does not change beyond random fluctuations.
This is expected, as WELDA with no replacement corresponds to standard LDA, where we observed the same topic coherence values around $42.5$.
However, the same bad topic coherence values can be observered when $\lambda = 1.0$, i.e. when all words are replaced.
This indicates, that just relying on the word embeddings to achieve good topics and completely disregarding the standard LDA structure, does not work.
Intuitively, when all words are resampled, the means of the topics roughly stay the same after each iteration, as no knowledge from the text is incorporated into the model anymore.
This results in roughly constant values for $\lambda = 1.0$.

All the other $\lambda$ parameters from $0.1$ to $0.9$ follow a clearly rising trend.
Around iteration $140$, the values tend to reach their optimum and stay there except some random fluctuations.
There is no clear correlation between $\lambda$ and the topic coherence recognizable.
The best values are reached with $\lambda = 0.73$ with a topic coherence of $0.474$, however the second best results are reached by a comparatively low $lambda = 0.2$ and topic coherence of $0.470$.
It seems and the Gibbs sampling gets stuck in a local optimum.
Thus, as in standard topic models, it seems best to rerun the model several times and pick the best.
This is an improvement of $0.051$ over the baseline value.

We now present the results for running with the embeddings trained on Wikipedia.
Interestingly, the topic coherence values are lower on average when using this embeddings.
This goes against the intuition, because topic coherence is evaluated using Wikipedia as a reference corpus.
One would expect, that knowledge from the Wikipedia corpus leaks into the topics during resampling, and thus leads to better topic models.
However, this is not the case.
The observations for $\lambda = 0.0$ and $\lambda = 1.0$ are the same as above.
However, here also $\lambda = 0.8$ and $\lambda = 0.9$ are significantly worse than the rest.
The reason for this is the same as above: resampling too often throws away the corpus and just operates on the embeddings.
The best results with this embeddings is only $0.458$ as compared to $0.474$ with the other embeddings.
This is only an improvement of $0.039$ over the baseline value.
% TODO: Write more?

\begin{figure}
       \centering
       \includegraphics[width=\textwidth]{figures/welda_gaussian_lambda_200.png}
       \caption{Topic coherence development for Gaussian WELDA and \textsc{Wikipedia} as embedding corpus. Topic coherence was evaluated every 20 iterations.}
       \label{fig:welda_gaussian_lambda_200}
\end{figure}

% TODO: Nips
We illustrate the change of topics in \textsc{Nips} and \textsc{20news} in Table~\ref{table:topic_development}.
\begin{table}[]
  \centering
  \caption{Topic evolution from standard LDA to Gaussian WELDA after 200 iterations. The crossed out words were in the original top ten words of LDA, but are not in the top topic words after WELDA. The bold words are the new words. The top three topics are from the \textsc{20news} corpus, the lower three from the \textsc{Nips} corpus. The examples clearly show WELDA's capability to add specific, meaningful words in the top words, while at the same time removing overly general words.}
  \label{table:topic_development}
  \begin{tabular}{p{13.5cm}}
    \hline
    god \st{believe} \st{one} evidence religion \st{people} argument exist \st{question} atheists \textbf{faith belief existence life} \\
    \hline
    year players baseball \st{article} \st{good} \st{writes} lopez \st{jewish} league average \textbf{hit braves season hitter} \\
    \hline
    information list mail \st{may} internet send anonymous \st{faq} email \st{use} \textbf{system available privacy} \\
    \hline
    \hline
    speech recognition speaker gamma acoustic phoneme \st{time} vowel \st{information} speakers \textbf{spectral phonetic} \\
    \hline
    bayesian gaussian distribution prior posterior \st{data} \st{using} evidence \st{mean} \st{sampling} \textbf{random covariance monte carlo} \\
    \hline
    kernel algorithm vector support loss margin \st{set} \st{function} linear examples \textbf{svm boosting} \\
    \hline
  \end{tabular}
\end{table}

\subsubsection{Influence of the number of PCA dimensions $N_{PCA}$}
\subsubsection{Influence of the number of distribution estimation samples $N_{TOP}$}
\subsubsection{Influence of the background topic}
\subsubsection{Running LDA as a prerequisite}
Normally, we run standard LDA before we start with WELDA algorithm for further topic refining.
This is because the topic distributions are estimated on the top words of each topic.
If we run WELDA from the first iteration, with topics assigned randomly and uniformly before the first iteration, the distributions are not well spread over the embedding space.
For example, here are the top ten words of the three most-occurring topics after random initialization: \\
\hspace*{0cm} \framebox[0.8\textwidth]{would one writes article like people get know also use} \\
\hspace*{0cm} \framebox[0.8\textwidth]{would one writes article like people get know also use} \\
\hspace*{0cm} \framebox[0.8\textwidth]{would one writes article people like max know also think} \\

These are the words the distributions are estimated on.
This initial setup has a topic coherence value of $TC = 0.311$.
Now, instead of improving clearly defined topics by sampling new words near the well-spread topic means, we end up actually generating more overly general, low information words for all topics.
To prove this thought experiment, we run WELDA with randomly initialized topics for $1500$ iterations on the \textsc{20News} corpus with $\lambda = 0.2$ and $\lambda = 0.5$.
The results are displayed in Figure~\ref{fig:standard_lda_as_prerequisite}.
As expected, TODO

\begin{figure}
       \centering
       \includegraphics[width=\textwidth]{figures/standard_lda_alpha_beta.png}
       \caption{TODO.}
       \label{fig:standard_lda_as_prerequisite}
\end{figure}



\subsubsection{Choice of word embeddings}
\subsubsection{Top word selection}

\subsubsection{Overfitting with high lambda}
When the words are spread far apart, and the mean is in the middle of nowhere, things tend to overfit.
/data/wikipedia/2016-06-21/topic-models/topic.20news.50-1500.alpha-0-02.beta-0-02/model.20news.dim-50.skip-gram.embedding.welda.gaussian.distance-cos.lambda-1-0/welda.iteration-200.topics



\section{Document Classification}
We also evaluated topic models on another commonly used task: document classification.
In contrast to topic coherence, this allows to use topic models in an supervised setting with clear evaluation metrics.
Given a set of documents, which belong to different semantic groups, the task of document classification is to correctly classify unseen documents into these groups.

We use the document-topic matrices.

\subsection{Experiment setup}
\subsection{Results}

\chapter{Comparative evaluation of word-embedding-improved topic models}
\section{Training word-embeddings}

We trained our own word embeddings using the gensim package by {\v R}eh{\r u}{\v r}ek~\cite{Rehurek2010}, written in Python.
We used skip-gram training, with a window size of five, negative sampling with ten noise words, and running training for twenty iterations.

For preprocessing, we split the corpora into sentences.
Words were not lowercased and no stopwords were removed, as both have been show to harm the performance of the resulting vectors.
% TODO: Quote

We also compared to word embedding vectors published by Google\footnote{\url{https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing}}.
We evaluated using analogy reasoning tasks published by Mikolov et al.~\cite{Mikolov2013}.
The results can be seen in Table~\ref{table:word_embeddings_performance}.
% TODO: Explain analogy reasoning

\begin{table}[]
  \centering
  \caption{Performance}
  \label{table:word_embeddings_performance}
  \begin{tabular}{ll}
  \hline
                                                                  & \textbf{Accuracy} \\ \hline
  \textbf{200 dimensions, trained on English Wikipedia}           & 56.5 \%           \\
  \textbf{50-dimensions, trained on 20-news corpus}               & 11.1 \%           \\
  \textbf{Embeddings published by Google, trained on Google News} & 74.3 \%
  \end{tabular}
\end{table}


\section{Topic coherence}
\subsection{Raw word-embedding topic model}
As a baseline, we start with a topic model solely based on the word-similarity in the embedding space.
This way, we see whether using the most similar words for a given word already yields good topics, or whether standard LDA

For this, we used the standard \emph{distance} function, that is provided in the \emph{word2vec} package.
Given a word or a list of words, this outputs the top $n$ similar words in the embedding space.
In the original implementation, the search words are never in the output.
We modified this, so that the search words may also be in the output.
For one search word, the search word is always in the output, as the similarity is $1$.

As search words, we took the top topic words of a standard LDA run with $iterations=1500, \alpha=0.02, \beta=0.02$.
We experimented with two cases: only taking the top word and taking the average of all top ten words.
For example, for the search word ``game'', the topic (in the 200-dimensional embeddings trained on Wikipedia) is: \\
\hspace*{0cm} \framebox[1.03\width]{game games skullgirls tetris gameplay battletoads puzzle 2player picross multiplayer} \\

We show the results in Table~\ref{table:raw_we_tm}.
The results are worse than running LDA, thus showing the only taking word embeddings is not enough.
\begin{table}[]
  \centering
  \caption{Topic coherence on the \textsc{20news} corpus when using word embedding similarity for topics. The results are slightly worse than standard LDA.}
  \label{table:raw_we_tm}
  \begin{tabular}{ccc}
    \textbf{search word}     & \textbf{200-dim., Wikipedia} & \textbf{50-dim., 20news} \\
    \hline
    top word                 & 0.414                        & 0.410                    \\
    average of top ten words & 0.419                        & 0.389
  \end{tabular}
\end{table}

% TODO: Mention later, that only the combination works good
\section{Document classification}
\section{Word similarity}


\section{Runtime}

\chapter{Conclusion and future work}

\chapter{Thoughts}
\section{Other notes}

Make a full chapter about Topic Model Evaluation with the paper Exploring the Space of Topic Coherence Measures

Concept categorization
Stopword removal, lowercase, build matrix, stemming
Train mallet
Visualize in tsne
Hyperparameter optimization
Motivation: topic with countries
Paper: Schwächen, Stärken, ausführlich

TC evaluation for all models!
Spherical: Is it fair to evaluate with different topic nr?
Strange words in gaussian: yes

Run WELDA from scratch

Beim resampling: sanity check, ABER: optimiert auf, Wort nur in 1 topic

topic boost analysis

curse of dimensionality for nearest neighbour search
fewer: dimensions: slower runtime, loosing information; more dimensions, usually only one cluster because of CoD

\clearpage
\printbibliography

\begin{appendices}
%\include{chapters/appendix}
\end{appendices}

%\include{chapters/eigenstaendigkeit}
\end{document}
